{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mazes \n",
    "\n",
    "Now we explore how to use the Markov Decision Process framework to apply RL to 2D mazes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Learn to move\n",
    "\n",
    "We consider a 4 x 4 grid with the upper-left and bottom-right corners that are respectively the initial and the terminal states.\n",
    "Our goal is to reach as fast as possible the terminal state starting from the initial one.\n",
    "\n",
    "\n",
    "| / | 0| 1 | 2 | 3 |\n",
    "| --- | --- | --- | --- |--- |\n",
    "|0|a|b|c|d|\n",
    "|1|e|f|g|h|\n",
    "|2|i|l|m|n|\n",
    "|3|o|p|q|r|\n",
    "\n",
    "Moves:\n",
    "- 'up' = 0\n",
    "- 'down' = 1\n",
    "- 'left' = 2\n",
    "- 'right' = 3\n",
    "\n",
    "Border conditions: wall of the domain (i.e. no toroidal border conditions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 16 states and 4 possible actions, thus the action-state space is of 64 (this is comparable in complexity of learning to a 64 armed bandits). Furthermore, each action has a deterministic effect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# those are the functions for the environment\n",
    "def reward(s_new):\n",
    "    if s_new == 15:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "def check_border(pos):\n",
    "    y = pos[0]\n",
    "    x = pos[1]\n",
    "    if x >= 0 and x <= 3 and y >= 0 and y <= 3:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def state_to_pos(s):\n",
    "    YX = np.arange(0,16).reshape((4,4))\n",
    "    pos = np.array([np.where(s == YX)[0][0],np.where(s == YX)[1][0]])\n",
    "    return pos\n",
    "\n",
    "def pos_to_state(pos):\n",
    "    YX = np.arange(0,16).reshape((4,4))\n",
    "    state = YX[pos[0],pos[1]] # works both if pos is a list of scalars or if it is a tuple\n",
    "    return state\n",
    "\n",
    "def turn(a,s):\n",
    "    \"\"\"Takes action and current state, returns the new state s_new and the reward for the last move.\n",
    "        Use (x,y) = s, with x,y = 0,...,3.\n",
    "    \"\"\"\n",
    "    # first y and then x, because of rows and columns access \n",
    "    # y selects the row, x the column, thus changing y moves up or down, changing x left or right\n",
    "    moves = {0:[1,0],1:[-1,0], 2:[0,-1], 3:[0,1]} \n",
    "    #print(\"Direction of movement: \", moves[a])\n",
    "    pos = state_to_pos(s)\n",
    "    #print(\"Starting position: \", pos)\n",
    "    pos_temp = pos + np.array(moves[a])\n",
    "    #print(\"Position attempted to reach: \", pos_temp)\n",
    "    if check_border(pos_temp):\n",
    "        #print(\"Postion allowed.\")\n",
    "        pos_new = pos_temp\n",
    "    else:\n",
    "        #print(\"Postion negated.\")\n",
    "        pos_new = pos\n",
    "    \n",
    "    s_new = pos_to_state(pos_new)\n",
    "    #r = reward(s_new)\n",
    "    r = reward(s)\n",
    "    \n",
    "    return s_new, r   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update rule:\n",
    "\n",
    "$q_{k+1}(s,a) = r(s,a) + \\gamma q_{k}(s',a')$, where a' is the state choosen at that iteration by our policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for the agent\n",
    "def greedy_pol_v0(s, q_values):\n",
    "    return np.argmax(q_values[s])\n",
    "\n",
    "def update_q_v0(s, a, r, sp, ap, q_values, gamma = 1):\n",
    "    q_values[s,a] = r + gamma*q_values[sp,ap]\n",
    "    return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminal state reached at step 68.\n",
      "Terminal state reached at step 21.\n",
      "Terminal state reached at step 20.\n",
      "Terminal state reached at step 7.\n",
      "Terminal state reached at step 8.\n",
      "Terminal state reached at step 38.\n",
      "Terminal state reached at step 8.\n",
      "Terminal state reached at step 14.\n",
      "Terminal state reached at step 12.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 8.\n",
      "Terminal state reached at step 10.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 7.\n",
      "Terminal state reached at step 9.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 8.\n",
      "Terminal state reached at step 9.\n",
      "Terminal state reached at step 11.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Game solved after 2 epochs.\n"
     ]
    }
   ],
   "source": [
    "# inizialization of the system\n",
    "grid_size = (4,4) # the environment at the moment works only for grid = 4x4\n",
    "n_states = grid_size[0]*grid_size[1]\n",
    "n_actions = 4\n",
    "discount_factor = 1 # undiscounted, we can even omit it\n",
    "n_batch = 100\n",
    "epochs = 0\n",
    "q_values = np.zeros((n_states,n_actions)) #initialize to zero\n",
    "\n",
    "while True:\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@\n",
    "    # here starts an epoch\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@\n",
    "    epochs = epochs + 1\n",
    "    game_progress = np.zeros(n_batch) # bunch of 100 episodes\n",
    "    \n",
    "    for i in range(n_batch):\n",
    "        #@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "        # here starts an episode\n",
    "        #@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "        s = 0 # init state\n",
    "        steps = 0\n",
    "\n",
    "        while True:\n",
    "            steps = steps + 1\n",
    "            #print(\"Current state: \", s)\n",
    "            a = greedy_pol_v0(s, q_values)\n",
    "            #print(\"Action choosen: \", a)\n",
    "            sp, r = turn(a,s) # interacts with the environment \n",
    "            #print(\"New state : \", sp)\n",
    "            #print(\"Reward obtained : \", r)\n",
    "            a_temp = greedy_pol_v0(sp, q_values)\n",
    "            #print(\"Action likely to choose in the new state: \", a_temp) \n",
    "            q_values = update_q_v0(s, a, r, sp, a_temp, q_values, gamma = discount_factor)\n",
    "            # update states\n",
    "            s = sp\n",
    "\n",
    "            if s == 15:\n",
    "                print(\"Terminal state reached at step %d.\"%steps)\n",
    "                game_progress[i] = steps\n",
    "                break\n",
    "            if steps >= 100:\n",
    "                print(\"Too much time has passed. Game Over.\")\n",
    "                game_progress[i] = steps\n",
    "                break\n",
    "    \n",
    "    if game_progress.mean() <= 6.1: #6 is the optimal number of moves\n",
    "        print(\"Game solved after %d epochs.\"%epochs)\n",
    "        break\n",
    "    if epochs > 100:\n",
    "        print(\"Hey, you're too slow to solve it!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(q_values.max(axis=1).reshape((4,4)))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Learn to avoid obstacles\n",
    "\n",
    "Legend:\n",
    "\n",
    "___ = free to go\n",
    "\n",
    "*** = obstacle\n",
    "\n",
    "| / | 0| 1 | 2 | 3 |\n",
    "| --- | --- | --- | --- |--- |\n",
    "|0    |start|___ |*** |*** |\n",
    "|1    | ___ |___ |___ |___ |\n",
    "|2    | ___ |*** |*** |___ |\n",
    "|3    | ___ |___ |*** |end |\n",
    "\n",
    "To do that we use the same environment, but modify the reward function, so that if the state that we are trying to reach is a wall the reward will be -1, else -0.05, except for the terminal state that will be 0.\n",
    "\n",
    "Here we also do two changes w.r.t. the previous case: we use an $\\epsilon$-greedy policy, with $\\epsilon=0.01$, and we reward the agent evaluating $s$ instead of $s'$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_v1(s):\n",
    "    if s == 15:\n",
    "        return 0\n",
    "    elif ((s == 2) or (s == 3) or (s == 9) or (s == 10) or (s == 14)):\n",
    "        return -1\n",
    "    else:\n",
    "        return -0.05\n",
    "    \n",
    "def turn_v1(a,s):\n",
    "    \"\"\"Takes action and current state, returns the new state s_new and the reward for the last move.\n",
    "        Use (x,y) = s, with x,y = 0,...,3.\n",
    "    \"\"\"\n",
    "    r = reward_v1(s)\n",
    "    # first y and then x, because of rows and columns access \n",
    "    # y selects the row, x the column, thus changing y moves up or down, changing x left or right\n",
    "    moves = {0:[1,0],1:[-1,0], 2:[0,-1], 3:[0,1]} \n",
    "    #print(\"Direction of movement: \", moves[a])\n",
    "    pos = state_to_pos(s)\n",
    "    #print(\"Starting position: \", pos)\n",
    "    pos_temp = pos + np.array(moves[a])\n",
    "    #print(\"Position attempted to reach: \", pos_temp)\n",
    "    if check_border(pos_temp):\n",
    "        #print(\"Postion allowed.\")\n",
    "        pos_new = pos_temp\n",
    "    else:\n",
    "        #print(\"Postion negated.\")\n",
    "        pos_new = pos\n",
    "    \n",
    "    s_new = pos_to_state(pos_new)\n",
    "    \n",
    "    return s_new, r   \n",
    "\n",
    "def e_greedy_pol_v0(s, q_values, eps = 0.2):\n",
    "    u = np.random.rand()\n",
    "    if u > eps:\n",
    "        return np.argmax(q_values[s])\n",
    "    else:\n",
    "        return np.random.randint(0, len(q_values[s]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminal state reached at step 68.\n",
      "Terminal state reached at step 21.\n",
      "Terminal state reached at step 20.\n",
      "Terminal state reached at step 7.\n",
      "Terminal state reached at step 13.\n",
      "Terminal state reached at step 44.\n",
      "Terminal state reached at step 9.\n",
      "Terminal state reached at step 9.\n",
      "Terminal state reached at step 10.\n",
      "Terminal state reached at step 35.\n",
      "Terminal state reached at step 11.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 11.\n",
      "Terminal state reached at step 7.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 7.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 8.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 18.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 8.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 8.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 8.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 10.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Terminal state reached at step 6.\n",
      "Game solved after 2 epochs.\n"
     ]
    }
   ],
   "source": [
    "# inizialization of the system\n",
    "grid_size = (4,4) # the environment at the moment works only for grid = 4x4\n",
    "n_states = grid_size[0]*grid_size[1]\n",
    "n_actions = 4\n",
    "discount_factor = 1 # undiscounted, we can even omit it\n",
    "n_batch = 100\n",
    "epochs = 0\n",
    "q_values = np.zeros((n_states,n_actions)) #initialize to zero\n",
    "\n",
    "while True:\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@\n",
    "    # here starts an epoch\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@\n",
    "    epochs = epochs + 1\n",
    "    game_progress = np.zeros(n_batch) # bunch of 100 episodes\n",
    "    \n",
    "    for i in range(n_batch):\n",
    "        #@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "        # here starts an episode\n",
    "        #@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "        s = 0 # init state\n",
    "        steps = 0\n",
    "\n",
    "        while True:\n",
    "            steps = steps + 1\n",
    "            #print(\"Current state: \", s)\n",
    "            a = e_greedy_pol_v0(s, q_values, eps = 0.01)\n",
    "            #print(\"Action choosen: \", a)\n",
    "            sp, r = turn_v1(a,s) # interacts with the environment \n",
    "            #print(\"New state : \", sp)\n",
    "            #print(\"Reward obtained : \", r)\n",
    "            a_temp = greedy_pol_v0(sp, q_values)\n",
    "            #print(\"Action likely to choose in the new state: \", a_temp) \n",
    "            q_values = update_q_v0(s, a, r, sp, a_temp, q_values, gamma = discount_factor)\n",
    "            # update states\n",
    "            s = sp\n",
    "\n",
    "            if s == 15:\n",
    "                print(\"Terminal state reached at step %d.\"%steps)\n",
    "                game_progress[i] = steps\n",
    "                break\n",
    "            if steps >= 100:\n",
    "                print(\"Too much time has passed. Game Over.\")\n",
    "                game_progress[i] = steps\n",
    "                break\n",
    "    \n",
    "    if game_progress.mean() <= 6.2: #6 is the optimal number of moves\n",
    "        print(\"Game solved after %d epochs.\"%epochs)\n",
    "        break\n",
    "    if epochs > 100:\n",
    "        print(\"Hey, you're too slow to solve it!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(q_values.max(axis=1).reshape((4,4)))\n",
    "plt.colorbar()\n",
    "plt.title(\"Value map\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Double agent\n",
    "Now we want to see how two agents can coordinate to reach two possible terminal states.\n",
    "We start the game with one agent at each of the top corners of the grid and two terminal states.\n",
    "If the two agents collide it's game over. Each agent can go to one of the two terminal states, but they can not occupy the same one to win the game. The ideal to test if they learn an optimal policy is to locate the two terminal states so that there exist both a suboptimal and an optimal solution.\n",
    "\n",
    "Consider for example this maze:\n",
    "\n",
    "| / | 0| 1 | 2 | 3 | 4 |\n",
    "| --- | --- | --- | --- |--- |--- |\n",
    "|0    |S1   |*** |T1  |___ |S2  |\n",
    "|1    | ___ |*** |___ |___ |___ |\n",
    "|2    | ___ |___ |___ |___ |*** |\n",
    "|3    | ___ |___ |___ |___ |*** |\n",
    "|4    | ___ |___ |___ |___ |T2  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single states: \n",
      " [[ 0  1  2  3  4]\n",
      " [ 5  6  7  8  9]\n",
      " [10 11 12 13 14]\n",
      " [15 16 17 18 19]\n",
      " [20 21 22 23 24]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Single states: \\n\", np.arange(0,25).reshape((5,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "STD_REWARD = -0.5\n",
    "CRUSH_REWARD = -10\n",
    "WALL_REWARD = -10\n",
    "\n",
    "# those are the functions for the environment\n",
    "def one_reward(s):\n",
    "    \"\"\"Reward for the state of a single agent.\"\"\"\n",
    "    if (s == 2) or (s == 24):\n",
    "        return 0\n",
    "    elif ((s == 1) or (s == 6) or (s == 14) or (s == 19)):\n",
    "        return WALL_REWARD\n",
    "    else:\n",
    "        return STD_REWARD\n",
    "    \n",
    "def check_border_v1(pos):\n",
    "    \"\"\"5x5 map without toridal border conditions.\"\"\"\n",
    "    y = pos[0]\n",
    "    x = pos[1]\n",
    "    if x >= 0 and x <= 4 and y >= 0 and y <= 4:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def state_to_pos_v1(s):\n",
    "    YX = np.arange(0,25).reshape((5,5))\n",
    "    pos = np.array([np.where(s == YX)[0][0],np.where(s == YX)[1][0]])\n",
    "    return pos\n",
    "\n",
    "def pos_to_state_v1(pos):\n",
    "    YX = np.arange(0,25).reshape((5,5))\n",
    "    state = YX[pos[0],pos[1]] # works both if pos is a list of scalars or if it is a tuple\n",
    "    return state\n",
    "\n",
    "def turn_v2(s, a):\n",
    "    \"\"\"Takes action and current state, returns the new state s_new and the reward for the last move.\n",
    "        Use (x,y) = s, with x,y = 0,...,4.\n",
    "        Now both a and b are list of two components, one for each agent.\n",
    "    \"\"\"\n",
    "    valid_move = True\n",
    "    # first y and then x, because of rows and columns access \n",
    "    # y selects the row, x the column, thus changing y moves up or down, changing x left or right\n",
    "    moves = {0:[1,0],1:[-1,0], 2:[0,-1], 3:[0,1]} \n",
    "    #@@@@@@@@@@@@\n",
    "    # first agent\n",
    "    #@@@@@@@@@@@@\n",
    "    #print(\"States: \", s)\n",
    "    #print(\"Actions: \", a)\n",
    "    r0 = one_reward(s[0])\n",
    "    #print(\"Direction of movement: \", moves[a[0]])\n",
    "    pos0 = state_to_pos_v1(s[0])\n",
    "    #print(\"Starting position: \", pos0)\n",
    "    pos_temp0 = pos0 + np.array(moves[a[0]])\n",
    "    #print(\"Position attempted to reach: \", pos_temp0)\n",
    "    if check_border_v1(pos_temp0):\n",
    "        #print(\"Postion allowed.\")\n",
    "        pos_new0 = pos_temp0\n",
    "    else:\n",
    "        #print(\"Postion negated.\")\n",
    "        pos_new0 = pos0\n",
    "    \n",
    "    s_new0 = pos_to_state_v1(pos_new0)\n",
    "    \n",
    "    #@@@@@@@@@@@@@\n",
    "    # second agent\n",
    "    #@@@@@@@@@@@@@\n",
    "    \n",
    "    if s[1] == s[0]:\n",
    "        r1 = CRUSH_REWARD\n",
    "    else:\n",
    "        r1 = one_reward(s[1])\n",
    "        \n",
    "    #print(\"Direction of movement: \", moves[a[1]])\n",
    "    #print(\"State s1: \", s[1])\n",
    "    pos1 = state_to_pos_v1(s[1])\n",
    "    #print(\"Starting position: \", pos1)\n",
    "    pos_temp1 = pos1 + np.array(moves[a[1]])\n",
    "    #print(\"Position attempted to reach: \", pos_temp1)\n",
    "    if check_border_v1(pos_temp1):\n",
    "        #print(\"Postion allowed.\")\n",
    "        pos_new1 = pos_temp1\n",
    "    else:\n",
    "        #print(\"Postion negated.\")\n",
    "        pos_new1 = pos1\n",
    "    \n",
    "    s_new1 = pos_to_state_v1(pos_new1)\n",
    "    \n",
    "    s_new = [s_new0,s_new1]\n",
    "    \n",
    "    if r0 == 0 and r1 != 0:\n",
    "        r0  = STD_REWARD\n",
    "    if  r1 == 0 and r0 != 0:\n",
    "        r1  = STD_REWARD\n",
    "        \n",
    "    r = r0 + r1\n",
    "    \n",
    "    return s_new, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy_pol_v1(s, q_values, eps = 0.01):\n",
    "    # s is encoded in input, a is encoded in output\n",
    "    u = np.random.rand()\n",
    "    if u > eps:\n",
    "        return np.argmax(q_values[s])\n",
    "    else:\n",
    "        return np.random.randint(0, len(q_values[s]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(v_dec, L):\n",
    "    # v_two = [v1,v2]\n",
    "    # returns the encoded version V[v1,v2] of V = np.arange(0,L)\n",
    "    # L = length(all_possible_v)\n",
    "    V = np.arange(0,L**2).reshape((L,L))\n",
    "    v_enc = V[v_dec[0],v_dec[1]] \n",
    "    return v_enc\n",
    "\n",
    "def decode(v_enc, L):\n",
    "    V = np.arange(0,L**2).reshape((L,L))\n",
    "    v_dec = np.array([np.where(v_enc == V)[0][0],np.where(v_enc == V)[1][0]])\n",
    "    return v_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([5, 0], -10.5)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turn_v2([0,1],[0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24, 24])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_two = [24,24] # init state: top corners\n",
    "s = encode(s_two, 25)\n",
    "decode(s, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average step to solve epoch 1:  94.2\n",
      "Average reward per episode in epoch 1:  -373.975\n",
      "Average terminal state for agent 1:  15.58\n",
      "Average terminal state for agent 2:  11.75 \n",
      "\n",
      "Average step to solve epoch 2:  92.29\n",
      "Average reward per episode in epoch 2:  -385.46\n",
      "Average terminal state for agent 1:  11.92\n",
      "Average terminal state for agent 2:  13.89 \n",
      "\n",
      "Average step to solve epoch 3:  77.73\n",
      "Average reward per episode in epoch 3:  -313.425\n",
      "Average terminal state for agent 1:  12.47\n",
      "Average terminal state for agent 2:  12.21 \n",
      "\n",
      "Average step to solve epoch 4:  61.06\n",
      "Average reward per episode in epoch 4:  -183.99\n",
      "Average terminal state for agent 1:  15.1\n",
      "Average terminal state for agent 2:  11.14 \n",
      "\n",
      "Average step to solve epoch 5:  54.26\n",
      "Average reward per episode in epoch 5:  -133.205\n",
      "Average terminal state for agent 1:  13.98\n",
      "Average terminal state for agent 2:  11.73 \n",
      "\n",
      "Average step to solve epoch 6:  37.61\n",
      "Average reward per episode in epoch 6:  -84.16\n",
      "Average terminal state for agent 1:  15.61\n",
      "Average terminal state for agent 2:  10.77 \n",
      "\n",
      "Average step to solve epoch 7:  26.98\n",
      "Average reward per episode in epoch 7:  -62.32\n",
      "Average terminal state for agent 1:  13.88\n",
      "Average terminal state for agent 2:  12.12 \n",
      "\n",
      "Average step to solve epoch 8:  22.43\n",
      "Average reward per episode in epoch 8:  -53.115\n",
      "Average terminal state for agent 1:  12.21\n",
      "Average terminal state for agent 2:  13.82 \n",
      "\n",
      "Average step to solve epoch 9:  16.84\n",
      "Average reward per episode in epoch 9:  -35.84\n",
      "Average terminal state for agent 1:  12.78\n",
      "Average terminal state for agent 2:  13.22 \n",
      "\n",
      "Average step to solve epoch 10:  18.29\n",
      "Average reward per episode in epoch 10:  -41.09\n",
      "Average terminal state for agent 1:  12.78\n",
      "Average terminal state for agent 2:  13.22 \n",
      "\n",
      "Average step to solve epoch 11:  16.49\n",
      "Average reward per episode in epoch 11:  -32.83\n",
      "Average terminal state for agent 1:  13.44\n",
      "Average terminal state for agent 2:  12.56 \n",
      "\n",
      "Average step to solve epoch 12:  15.07\n",
      "Average reward per episode in epoch 12:  -33.215\n",
      "Average terminal state for agent 1:  11.68\n",
      "Average terminal state for agent 2:  14.32 \n",
      "\n",
      "Average step to solve epoch 13:  17.03\n",
      "Average reward per episode in epoch 13:  -34.035\n",
      "Average terminal state for agent 1:  12.12\n",
      "Average terminal state for agent 2:  13.88 \n",
      "\n",
      "Average step to solve epoch 14:  15.58\n",
      "Average reward per episode in epoch 14:  -35.625\n",
      "Average terminal state for agent 1:  11.46\n",
      "Average terminal state for agent 2:  14.54 \n",
      "\n",
      "Average step to solve epoch 15:  14.04\n",
      "Average reward per episode in epoch 15:  -29.145\n",
      "Average terminal state for agent 1:  10.14\n",
      "Average terminal state for agent 2:  15.86 \n",
      "\n",
      "Average step to solve epoch 16:  12.91\n",
      "Average reward per episode in epoch 16:  -26.305\n",
      "Average terminal state for agent 1:  11.02\n",
      "Average terminal state for agent 2:  14.98 \n",
      "\n",
      "Average step to solve epoch 17:  14.01\n",
      "Average reward per episode in epoch 17:  -27.405\n",
      "Average terminal state for agent 1:  12.56\n",
      "Average terminal state for agent 2:  13.44 \n",
      "\n",
      "Average step to solve epoch 18:  12.96\n",
      "Average reward per episode in epoch 18:  -26.735\n",
      "Average terminal state for agent 1:  10.8\n",
      "Average terminal state for agent 2:  15.2 \n",
      "\n",
      "Average step to solve epoch 19:  13.33\n",
      "Average reward per episode in epoch 19:  -29.1\n",
      "Average terminal state for agent 1:  10.8\n",
      "Average terminal state for agent 2:  15.2 \n",
      "\n",
      "Average step to solve epoch 20:  13.04\n",
      "Average reward per episode in epoch 20:  -27.195\n",
      "Average terminal state for agent 1:  10.14\n",
      "Average terminal state for agent 2:  15.86 \n",
      "\n",
      "Average step to solve epoch 21:  13.4\n",
      "Average reward per episode in epoch 21:  -27.175\n",
      "Average terminal state for agent 1:  11.24\n",
      "Average terminal state for agent 2:  14.76 \n",
      "\n",
      "Average step to solve epoch 22:  13.24\n",
      "Average reward per episode in epoch 22:  -28.63\n",
      "Average terminal state for agent 1:  11.02\n",
      "Average terminal state for agent 2:  14.98 \n",
      "\n",
      "Average step to solve epoch 23:  14.11\n",
      "Average reward per episode in epoch 23:  -29.69\n",
      "Average terminal state for agent 1:  12.78\n",
      "Average terminal state for agent 2:  13.22 \n",
      "\n",
      "Average step to solve epoch 24:  12.21\n",
      "Average reward per episode in epoch 24:  -25.415\n",
      "Average terminal state for agent 1:  10.58\n",
      "Average terminal state for agent 2:  15.42 \n",
      "\n",
      "Average step to solve epoch 25:  12.08\n",
      "Average reward per episode in epoch 25:  -26.9\n",
      "Average terminal state for agent 1:  9.7\n",
      "Average terminal state for agent 2:  16.3 \n",
      "\n",
      "Average step to solve epoch 26:  12.41\n",
      "Average reward per episode in epoch 26:  -26.945\n",
      "Average terminal state for agent 1:  10.58\n",
      "Average terminal state for agent 2:  15.42 \n",
      "\n",
      "Average step to solve epoch 27:  11.14\n",
      "Average reward per episode in epoch 27:  -19.975\n",
      "Average terminal state for agent 1:  9.48\n",
      "Average terminal state for agent 2:  16.52 \n",
      "\n",
      "Average step to solve epoch 28:  12.06\n",
      "Average reward per episode in epoch 28:  -22.795\n",
      "Average terminal state for agent 1:  9.92\n",
      "Average terminal state for agent 2:  16.08 \n",
      "\n",
      "Average step to solve epoch 29:  11.64\n",
      "Average reward per episode in epoch 29:  -22.945\n",
      "Average terminal state for agent 1:  9.26\n",
      "Average terminal state for agent 2:  16.74 \n",
      "\n",
      "Average step to solve epoch 30:  11.87\n",
      "Average reward per episode in epoch 30:  -25.17\n",
      "Average terminal state for agent 1:  9.48\n",
      "Average terminal state for agent 2:  16.52 \n",
      "\n",
      "Average step to solve epoch 31:  12.33\n",
      "Average reward per episode in epoch 31:  -23.635\n",
      "Average terminal state for agent 1:  10.8\n",
      "Average terminal state for agent 2:  15.2 \n",
      "\n",
      "Average step to solve epoch 32:  10.98\n",
      "Average reward per episode in epoch 32:  -20.385\n",
      "Average terminal state for agent 1:  10.36\n",
      "Average terminal state for agent 2:  15.64 \n",
      "\n",
      "Average step to solve epoch 33:  11.89\n",
      "Average reward per episode in epoch 33:  -22.91\n",
      "Average terminal state for agent 1:  9.92\n",
      "Average terminal state for agent 2:  16.08 \n",
      "\n",
      "Average step to solve epoch 34:  11.48\n",
      "Average reward per episode in epoch 34:  -21.36\n",
      "Average terminal state for agent 1:  8.38\n",
      "Average terminal state for agent 2:  17.62 \n",
      "\n",
      "Average step to solve epoch 35:  11.93\n",
      "Average reward per episode in epoch 35:  -22.19\n",
      "Average terminal state for agent 1:  12.12\n",
      "Average terminal state for agent 2:  13.88 \n",
      "\n",
      "Average step to solve epoch 36:  11.44\n",
      "Average reward per episode in epoch 36:  -21.795\n",
      "Average terminal state for agent 1:  11.02\n",
      "Average terminal state for agent 2:  14.98 \n",
      "\n",
      "Average step to solve epoch 37:  10.99\n",
      "Average reward per episode in epoch 37:  -21.155\n",
      "Average terminal state for agent 1:  9.92\n",
      "Average terminal state for agent 2:  16.08 \n",
      "\n",
      "Average step to solve epoch 38:  10.63\n",
      "Average reward per episode in epoch 38:  -18.325\n",
      "Average terminal state for agent 1:  9.26\n",
      "Average terminal state for agent 2:  16.74 \n",
      "\n",
      "Average step to solve epoch 39:  10.4\n",
      "Average reward per episode in epoch 39:  -19.995\n",
      "Average terminal state for agent 1:  9.7\n",
      "Average terminal state for agent 2:  16.3 \n",
      "\n",
      "Average step to solve epoch 40:  11.49\n",
      "Average reward per episode in epoch 40:  -19.565\n",
      "Average terminal state for agent 1:  11.02\n",
      "Average terminal state for agent 2:  14.98 \n",
      "\n",
      "Average step to solve epoch 41:  10.55\n",
      "Average reward per episode in epoch 41:  -20.525\n",
      "Average terminal state for agent 1:  10.36\n",
      "Average terminal state for agent 2:  15.64 \n",
      "\n",
      "Average step to solve epoch 42:  10.4\n",
      "Average reward per episode in epoch 42:  -17.62\n",
      "Average terminal state for agent 1:  7.94\n",
      "Average terminal state for agent 2:  18.06 \n",
      "\n",
      "Average step to solve epoch 43:  10.28\n",
      "Average reward per episode in epoch 43:  -20.73\n",
      "Average terminal state for agent 1:  8.6\n",
      "Average terminal state for agent 2:  17.4 \n",
      "\n",
      "Average step to solve epoch 44:  10.6\n",
      "Average reward per episode in epoch 44:  -19.34\n",
      "Average terminal state for agent 1:  10.58\n",
      "Average terminal state for agent 2:  15.42 \n",
      "\n",
      "Average step to solve epoch 45:  10.56\n",
      "Average reward per episode in epoch 45:  -18.825\n",
      "Average terminal state for agent 1:  10.58\n",
      "Average terminal state for agent 2:  15.42 \n",
      "\n",
      "Average step to solve epoch 46:  10.03\n",
      "Average reward per episode in epoch 46:  -16.68\n",
      "Average terminal state for agent 1:  10.14\n",
      "Average terminal state for agent 2:  15.86 \n",
      "\n",
      "Average step to solve epoch 47:  11.1\n",
      "Average reward per episode in epoch 47:  -21.265\n",
      "Average terminal state for agent 1:  8.82\n",
      "Average terminal state for agent 2:  17.18 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average step to solve epoch 48:  9.93\n",
      "Average reward per episode in epoch 48:  -18.29\n",
      "Average terminal state for agent 1:  9.04\n",
      "Average terminal state for agent 2:  16.96 \n",
      "\n",
      "Average step to solve epoch 49:  10.51\n",
      "Average reward per episode in epoch 49:  -19.345\n",
      "Average terminal state for agent 1:  10.14\n",
      "Average terminal state for agent 2:  15.86 \n",
      "\n",
      "Average step to solve epoch 50:  10.1\n",
      "Average reward per episode in epoch 50:  -16.37\n",
      "Average terminal state for agent 1:  10.58\n",
      "Average terminal state for agent 2:  15.42 \n",
      "\n",
      "Average step to solve epoch 51:  10.08\n",
      "Average reward per episode in epoch 51:  -16.54\n",
      "Average terminal state for agent 1:  9.48\n",
      "Average terminal state for agent 2:  16.52 \n",
      "\n",
      "Average step to solve epoch 52:  10.12\n",
      "Average reward per episode in epoch 52:  -19.24\n",
      "Average terminal state for agent 1:  9.26\n",
      "Average terminal state for agent 2:  16.74 \n",
      "\n",
      "Average step to solve epoch 53:  9.69\n",
      "Average reward per episode in epoch 53:  -18.335\n",
      "Average terminal state for agent 1:  8.6\n",
      "Average terminal state for agent 2:  17.4 \n",
      "\n",
      "Average step to solve epoch 54:  9.98\n",
      "Average reward per episode in epoch 54:  -17.865\n",
      "Average terminal state for agent 1:  7.94\n",
      "Average terminal state for agent 2:  18.06 \n",
      "\n",
      "Average step to solve epoch 55:  9.99\n",
      "Average reward per episode in epoch 55:  -17.21\n",
      "Average terminal state for agent 1:  9.7\n",
      "Average terminal state for agent 2:  16.3 \n",
      "\n",
      "Average step to solve epoch 56:  9.7\n",
      "Average reward per episode in epoch 56:  -15.78\n",
      "Average terminal state for agent 1:  8.82\n",
      "Average terminal state for agent 2:  17.18 \n",
      "\n",
      "Average step to solve epoch 57:  10.18\n",
      "Average reward per episode in epoch 57:  -18.73\n",
      "Average terminal state for agent 1:  9.48\n",
      "Average terminal state for agent 2:  16.52 \n",
      "\n",
      "Average step to solve epoch 58:  9.04\n",
      "Average reward per episode in epoch 58:  -14.74\n",
      "Average terminal state for agent 1:  8.38\n",
      "Average terminal state for agent 2:  17.62 \n",
      "\n",
      "Average step to solve epoch 59:  10.0\n",
      "Average reward per episode in epoch 59:  -16.555\n",
      "Average terminal state for agent 1:  10.36\n",
      "Average terminal state for agent 2:  15.64 \n",
      "\n",
      "Average step to solve epoch 60:  9.6\n",
      "Average reward per episode in epoch 60:  -16.06\n",
      "Average terminal state for agent 1:  11.46\n",
      "Average terminal state for agent 2:  14.54 \n",
      "\n",
      "Average step to solve epoch 61:  9.15\n",
      "Average reward per episode in epoch 61:  -15.99\n",
      "Average terminal state for agent 1:  8.16\n",
      "Average terminal state for agent 2:  17.84 \n",
      "\n",
      "Average step to solve epoch 62:  9.78\n",
      "Average reward per episode in epoch 62:  -16.905\n",
      "Average terminal state for agent 1:  8.6\n",
      "Average terminal state for agent 2:  17.4 \n",
      "\n",
      "Average step to solve epoch 63:  9.57\n",
      "Average reward per episode in epoch 63:  -18.5\n",
      "Average terminal state for agent 1:  9.04\n",
      "Average terminal state for agent 2:  16.96 \n",
      "\n",
      "Average step to solve epoch 64:  9.75\n",
      "Average reward per episode in epoch 64:  -18.775\n",
      "Average terminal state for agent 1:  8.6\n",
      "Average terminal state for agent 2:  17.4 \n",
      "\n",
      "Average step to solve epoch 65:  9.63\n",
      "Average reward per episode in epoch 65:  -14.57\n",
      "Average terminal state for agent 1:  8.6\n",
      "Average terminal state for agent 2:  17.4 \n",
      "\n",
      "Average step to solve epoch 66:  8.86\n",
      "Average reward per episode in epoch 66:  -16.84\n",
      "Average terminal state for agent 1:  8.38\n",
      "Average terminal state for agent 2:  17.62 \n",
      "\n",
      "Average step to solve epoch 67:  9.41\n",
      "Average reward per episode in epoch 67:  -15.775\n",
      "Average terminal state for agent 1:  6.84\n",
      "Average terminal state for agent 2:  19.16 \n",
      "\n",
      "Average step to solve epoch 68:  9.7\n",
      "Average reward per episode in epoch 68:  -16.635\n",
      "Average terminal state for agent 1:  9.04\n",
      "Average terminal state for agent 2:  16.96 \n",
      "\n",
      "Average step to solve epoch 69:  9.11\n",
      "Average reward per episode in epoch 69:  -14.715\n",
      "Average terminal state for agent 1:  9.04\n",
      "Average terminal state for agent 2:  16.96 \n",
      "\n",
      "Average step to solve epoch 70:  9.88\n",
      "Average reward per episode in epoch 70:  -16.53\n",
      "Average terminal state for agent 1:  8.82\n",
      "Average terminal state for agent 2:  17.18 \n",
      "\n",
      "Average step to solve epoch 71:  8.98\n",
      "Average reward per episode in epoch 71:  -15.44\n",
      "Average terminal state for agent 1:  7.94\n",
      "Average terminal state for agent 2:  18.06 \n",
      "\n",
      "Average step to solve epoch 72:  9.66\n",
      "Average reward per episode in epoch 72:  -17.26\n",
      "Average terminal state for agent 1:  9.48\n",
      "Average terminal state for agent 2:  16.52 \n",
      "\n",
      "Average step to solve epoch 73:  9.26\n",
      "Average reward per episode in epoch 73:  -16.1\n",
      "Average terminal state for agent 1:  7.28\n",
      "Average terminal state for agent 2:  18.72 \n",
      "\n",
      "Average step to solve epoch 74:  8.25\n",
      "Average reward per episode in epoch 74:  -13.665\n",
      "Average terminal state for agent 1:  5.52\n",
      "Average terminal state for agent 2:  20.48 \n",
      "\n",
      "Average step to solve epoch 75:  8.8\n",
      "Average reward per episode in epoch 75:  -14.215\n",
      "Average terminal state for agent 1:  8.16\n",
      "Average terminal state for agent 2:  17.84 \n",
      "\n",
      "Average step to solve epoch 76:  8.71\n",
      "Average reward per episode in epoch 76:  -14.79\n",
      "Average terminal state for agent 1:  7.5\n",
      "Average terminal state for agent 2:  18.5 \n",
      "\n",
      "Average step to solve epoch 77:  8.82\n",
      "Average reward per episode in epoch 77:  -14.14\n",
      "Average terminal state for agent 1:  7.94\n",
      "Average terminal state for agent 2:  18.06 \n",
      "\n",
      "Average step to solve epoch 78:  8.33\n",
      "Average reward per episode in epoch 78:  -12.035\n",
      "Average terminal state for agent 1:  7.94\n",
      "Average terminal state for agent 2:  18.06 \n",
      "\n",
      "Average step to solve epoch 79:  9.12\n",
      "Average reward per episode in epoch 79:  -14.82\n",
      "Average terminal state for agent 1:  11.24\n",
      "Average terminal state for agent 2:  14.76 \n",
      "\n",
      "Average step to solve epoch 80:  8.62\n",
      "Average reward per episode in epoch 80:  -13.56\n",
      "Average terminal state for agent 1:  5.74\n",
      "Average terminal state for agent 2:  20.26 \n",
      "\n",
      "Average step to solve epoch 81:  8.81\n",
      "Average reward per episode in epoch 81:  -13.845\n",
      "Average terminal state for agent 1:  8.82\n",
      "Average terminal state for agent 2:  17.18 \n",
      "\n",
      "Average step to solve epoch 82:  8.42\n",
      "Average reward per episode in epoch 82:  -13.265\n",
      "Average terminal state for agent 1:  6.4\n",
      "Average terminal state for agent 2:  19.6 \n",
      "\n",
      "Average step to solve epoch 83:  8.69\n",
      "Average reward per episode in epoch 83:  -13.25\n",
      "Average terminal state for agent 1:  8.16\n",
      "Average terminal state for agent 2:  17.84 \n",
      "\n",
      "Average step to solve epoch 84:  8.86\n",
      "Average reward per episode in epoch 84:  -14.275\n",
      "Average terminal state for agent 1:  6.62\n",
      "Average terminal state for agent 2:  19.38 \n",
      "\n",
      "Average step to solve epoch 85:  8.46\n",
      "Average reward per episode in epoch 85:  -13.4\n",
      "Average terminal state for agent 1:  7.28\n",
      "Average terminal state for agent 2:  18.72 \n",
      "\n",
      "Average step to solve epoch 86:  8.11\n",
      "Average reward per episode in epoch 86:  -12.575\n",
      "Average terminal state for agent 1:  7.06\n",
      "Average terminal state for agent 2:  18.94 \n",
      "\n",
      "Average step to solve epoch 87:  8.41\n",
      "Average reward per episode in epoch 87:  -13.54\n",
      "Average terminal state for agent 1:  6.62\n",
      "Average terminal state for agent 2:  19.38 \n",
      "\n",
      "Average step to solve epoch 88:  8.71\n",
      "Average reward per episode in epoch 88:  -14.125\n",
      "Average terminal state for agent 1:  8.82\n",
      "Average terminal state for agent 2:  17.18 \n",
      "\n",
      "Average step to solve epoch 89:  8.2\n",
      "Average reward per episode in epoch 89:  -13.045\n",
      "Average terminal state for agent 1:  6.18\n",
      "Average terminal state for agent 2:  19.82 \n",
      "\n",
      "Average step to solve epoch 90:  8.85\n",
      "Average reward per episode in epoch 90:  -13.22\n",
      "Average terminal state for agent 1:  7.5\n",
      "Average terminal state for agent 2:  18.5 \n",
      "\n",
      "Average step to solve epoch 91:  8.55\n",
      "Average reward per episode in epoch 91:  -12.92\n",
      "Average terminal state for agent 1:  7.28\n",
      "Average terminal state for agent 2:  18.72 \n",
      "\n",
      "Average step to solve epoch 92:  8.11\n",
      "Average reward per episode in epoch 92:  -11.815\n",
      "Average terminal state for agent 1:  8.16\n",
      "Average terminal state for agent 2:  17.84 \n",
      "\n",
      "Average step to solve epoch 93:  8.2\n",
      "Average reward per episode in epoch 93:  -12.475\n",
      "Average terminal state for agent 1:  7.94\n",
      "Average terminal state for agent 2:  18.06 \n",
      "\n",
      "Average step to solve epoch 94:  8.12\n",
      "Average reward per episode in epoch 94:  -12.585\n",
      "Average terminal state for agent 1:  6.62\n",
      "Average terminal state for agent 2:  19.38 \n",
      "\n",
      "Average step to solve epoch 95:  8.01\n",
      "Average reward per episode in epoch 95:  -11.81\n",
      "Average terminal state for agent 1:  7.28\n",
      "Average terminal state for agent 2:  18.72 \n",
      "\n",
      "Average step to solve epoch 96:  8.19\n",
      "Average reward per episode in epoch 96:  -12.845\n",
      "Average terminal state for agent 1:  6.18\n",
      "Average terminal state for agent 2:  19.82 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average step to solve epoch 97:  8.5\n",
      "Average reward per episode in epoch 97:  -13.06\n",
      "Average terminal state for agent 1:  6.62\n",
      "Average terminal state for agent 2:  19.38 \n",
      "\n",
      "Average step to solve epoch 98:  8.42\n",
      "Average reward per episode in epoch 98:  -12.79\n",
      "Average terminal state for agent 1:  6.62\n",
      "Average terminal state for agent 2:  19.38 \n",
      "\n",
      "Average step to solve epoch 99:  7.98\n",
      "Average reward per episode in epoch 99:  -12.16\n",
      "Average terminal state for agent 1:  7.06\n",
      "Average terminal state for agent 2:  18.94 \n",
      "\n",
      "Average step to solve epoch 100:  7.92\n",
      "Average reward per episode in epoch 100:  -10.58\n",
      "Average terminal state for agent 1:  7.72\n",
      "Average terminal state for agent 2:  18.28 \n",
      "\n",
      "Average step to solve epoch 101:  8.2\n",
      "Average reward per episode in epoch 101:  -12.665\n",
      "Average terminal state for agent 1:  6.4\n",
      "Average terminal state for agent 2:  19.6 \n",
      "\n",
      "Average step to solve epoch 102:  8.5\n",
      "Average reward per episode in epoch 102:  -12.49\n",
      "Average terminal state for agent 1:  7.94\n",
      "Average terminal state for agent 2:  18.06 \n",
      "\n",
      "Average step to solve epoch 103:  8.14\n",
      "Average reward per episode in epoch 103:  -11.085\n",
      "Average terminal state for agent 1:  7.28\n",
      "Average terminal state for agent 2:  18.72 \n",
      "\n",
      "Average step to solve epoch 104:  8.08\n",
      "Average reward per episode in epoch 104:  -12.07\n",
      "Average terminal state for agent 1:  7.06\n",
      "Average terminal state for agent 2:  18.94 \n",
      "\n",
      "Average step to solve epoch 105:  7.84\n",
      "Average reward per episode in epoch 105:  -11.07\n",
      "Average terminal state for agent 1:  6.62\n",
      "Average terminal state for agent 2:  19.38 \n",
      "\n",
      "Average step to solve epoch 106:  8.39\n",
      "Average reward per episode in epoch 106:  -13.615\n",
      "Average terminal state for agent 1:  7.28\n",
      "Average terminal state for agent 2:  18.72 \n",
      "\n",
      "Average step to solve epoch 107:  7.92\n",
      "Average reward per episode in epoch 107:  -11.53\n",
      "Average terminal state for agent 1:  7.06\n",
      "Average terminal state for agent 2:  18.94 \n",
      "\n",
      "Average step to solve epoch 108:  8.1\n",
      "Average reward per episode in epoch 108:  -12.28\n",
      "Average terminal state for agent 1:  7.06\n",
      "Average terminal state for agent 2:  18.94 \n",
      "\n",
      "Average step to solve epoch 109:  7.86\n",
      "Average reward per episode in epoch 109:  -11.28\n",
      "Average terminal state for agent 1:  5.08\n",
      "Average terminal state for agent 2:  20.92 \n",
      "\n",
      "Average step to solve epoch 110:  8.15\n",
      "Average reward per episode in epoch 110:  -11.665\n",
      "Average terminal state for agent 1:  7.72\n",
      "Average terminal state for agent 2:  18.28 \n",
      "\n",
      "Average step to solve epoch 111:  7.23\n",
      "Average reward per episode in epoch 111:  -10.08\n",
      "Average terminal state for agent 1:  4.64\n",
      "Average terminal state for agent 2:  21.36 \n",
      "\n",
      "Average step to solve epoch 112:  7.6\n",
      "Average reward per episode in epoch 112:  -11.4\n",
      "Average terminal state for agent 1:  4.86\n",
      "Average terminal state for agent 2:  21.14 \n",
      "\n",
      "Average step to solve epoch 113:  7.76\n",
      "Average reward per episode in epoch 113:  -11.56\n",
      "Average terminal state for agent 1:  5.3\n",
      "Average terminal state for agent 2:  20.7 \n",
      "\n",
      "Average step to solve epoch 114:  7.54\n",
      "Average reward per episode in epoch 114:  -12.1\n",
      "Average terminal state for agent 1:  5.08\n",
      "Average terminal state for agent 2:  20.92 \n",
      "\n",
      "Average step to solve epoch 115:  7.68\n",
      "Average reward per episode in epoch 115:  -11.195\n",
      "Average terminal state for agent 1:  5.08\n",
      "Average terminal state for agent 2:  20.92 \n",
      "\n",
      "Average step to solve epoch 116:  7.66\n",
      "Average reward per episode in epoch 116:  -10.225\n",
      "Average terminal state for agent 1:  6.84\n",
      "Average terminal state for agent 2:  19.16 \n",
      "\n",
      "Average step to solve epoch 117:  7.74\n",
      "Average reward per episode in epoch 117:  -12.11\n",
      "Average terminal state for agent 1:  5.08\n",
      "Average terminal state for agent 2:  20.92 \n",
      "\n",
      "Average step to solve epoch 118:  7.59\n",
      "Average reward per episode in epoch 118:  -10.535\n",
      "Average terminal state for agent 1:  5.96\n",
      "Average terminal state for agent 2:  20.04 \n",
      "\n",
      "Average step to solve epoch 119:  7.79\n",
      "Average reward per episode in epoch 119:  -10.925\n",
      "Average terminal state for agent 1:  5.3\n",
      "Average terminal state for agent 2:  20.7 \n",
      "\n",
      "Average step to solve epoch 120:  7.6\n",
      "Average reward per episode in epoch 120:  -11.02\n",
      "Average terminal state for agent 1:  5.52\n",
      "Average terminal state for agent 2:  20.48 \n",
      "\n",
      "Average step to solve epoch 121:  7.39\n",
      "Average reward per episode in epoch 121:  -10.525\n",
      "Average terminal state for agent 1:  3.98\n",
      "Average terminal state for agent 2:  22.02 \n",
      "\n",
      "Average step to solve epoch 122:  7.67\n",
      "Average reward per episode in epoch 122:  -10.995\n",
      "Average terminal state for agent 1:  5.52\n",
      "Average terminal state for agent 2:  20.48 \n",
      "\n",
      "Average step to solve epoch 123:  7.74\n",
      "Average reward per episode in epoch 123:  -11.54\n",
      "Average terminal state for agent 1:  6.18\n",
      "Average terminal state for agent 2:  19.82 \n",
      "\n",
      "Average step to solve epoch 124:  7.84\n",
      "Average reward per episode in epoch 124:  -11.07\n",
      "Average terminal state for agent 1:  7.06\n",
      "Average terminal state for agent 2:  18.94 \n",
      "\n",
      "Average step to solve epoch 125:  7.86\n",
      "Average reward per episode in epoch 125:  -10.14\n",
      "Average terminal state for agent 1:  6.18\n",
      "Average terminal state for agent 2:  19.82 \n",
      "\n",
      "Average step to solve epoch 126:  7.4\n",
      "Average reward per episode in epoch 126:  -9.87\n",
      "Average terminal state for agent 1:  5.3\n",
      "Average terminal state for agent 2:  20.7 \n",
      "\n",
      "Average step to solve epoch 127:  7.47\n",
      "Average reward per episode in epoch 127:  -10.605\n",
      "Average terminal state for agent 1:  4.86\n",
      "Average terminal state for agent 2:  21.14 \n",
      "\n",
      "Average step to solve epoch 128:  7.56\n",
      "Average reward per episode in epoch 128:  -9.935\n",
      "Average terminal state for agent 1:  5.52\n",
      "Average terminal state for agent 2:  20.48 \n",
      "\n",
      "Average step to solve epoch 129:  7.43\n",
      "Average reward per episode in epoch 129:  -9.9\n",
      "Average terminal state for agent 1:  5.08\n",
      "Average terminal state for agent 2:  20.92 \n",
      "\n",
      "Average step to solve epoch 130:  7.25\n",
      "Average reward per episode in epoch 130:  -9.435\n",
      "Average terminal state for agent 1:  5.08\n",
      "Average terminal state for agent 2:  20.92 \n",
      "\n",
      "Average step to solve epoch 131:  7.31\n",
      "Average reward per episode in epoch 131:  -9.21\n",
      "Average terminal state for agent 1:  4.64\n",
      "Average terminal state for agent 2:  21.36 \n",
      "\n",
      "Average step to solve epoch 132:  7.39\n",
      "Average reward per episode in epoch 132:  -9.765\n",
      "Average terminal state for agent 1:  4.86\n",
      "Average terminal state for agent 2:  21.14 \n",
      "\n",
      "Average step to solve epoch 133:  7.71\n",
      "Average reward per episode in epoch 133:  -11.13\n",
      "Average terminal state for agent 1:  6.4\n",
      "Average terminal state for agent 2:  19.6 \n",
      "\n",
      "Average step to solve epoch 134:  7.41\n",
      "Average reward per episode in epoch 134:  -9.785\n",
      "Average terminal state for agent 1:  6.4\n",
      "Average terminal state for agent 2:  19.6 \n",
      "\n",
      "Average step to solve epoch 135:  7.29\n",
      "Average reward per episode in epoch 135:  -10.425\n",
      "Average terminal state for agent 1:  5.3\n",
      "Average terminal state for agent 2:  20.7 \n",
      "\n",
      "Average step to solve epoch 136:  7.59\n",
      "Average reward per episode in epoch 136:  -10.82\n",
      "Average terminal state for agent 1:  6.84\n",
      "Average terminal state for agent 2:  19.16 \n",
      "\n",
      "Average step to solve epoch 137:  7.51\n",
      "Average reward per episode in epoch 137:  -10.93\n",
      "Average terminal state for agent 1:  5.3\n",
      "Average terminal state for agent 2:  20.7 \n",
      "\n",
      "Average step to solve epoch 138:  7.08\n",
      "Average reward per episode in epoch 138:  -9.17\n",
      "Average terminal state for agent 1:  5.3\n",
      "Average terminal state for agent 2:  20.7 \n",
      "\n",
      "Average step to solve epoch 139:  7.12\n",
      "Average reward per episode in epoch 139:  -9.305\n",
      "Average terminal state for agent 1:  4.86\n",
      "Average terminal state for agent 2:  21.14 \n",
      "\n",
      "Average step to solve epoch 140:  7.29\n",
      "Average reward per episode in epoch 140:  -10.805\n",
      "Average terminal state for agent 1:  4.42\n",
      "Average terminal state for agent 2:  21.58 \n",
      "\n",
      "Average step to solve epoch 141:  7.5\n",
      "Average reward per episode in epoch 141:  -11.205\n",
      "Average terminal state for agent 1:  5.52\n",
      "Average terminal state for agent 2:  20.48 \n",
      "\n",
      "Average step to solve epoch 142:  7.36\n",
      "Average reward per episode in epoch 142:  -10.115\n",
      "Average terminal state for agent 1:  5.08\n",
      "Average terminal state for agent 2:  20.92 \n",
      "\n",
      "Average step to solve epoch 143:  7.14\n",
      "Average reward per episode in epoch 143:  -9.42\n",
      "Average terminal state for agent 1:  5.74\n",
      "Average terminal state for agent 2:  20.26 \n",
      "\n",
      "Average step to solve epoch 144:  7.37\n",
      "Average reward per episode in epoch 144:  -10.03\n",
      "Average terminal state for agent 1:  3.76\n",
      "Average terminal state for agent 2:  22.24 \n",
      "\n",
      "Average step to solve epoch 145:  7.34\n",
      "Average reward per episode in epoch 145:  -10.95\n",
      "Average terminal state for agent 1:  3.32\n",
      "Average terminal state for agent 2:  22.68 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average step to solve epoch 146:  7.18\n",
      "Average reward per episode in epoch 146:  -10.315\n",
      "Average terminal state for agent 1:  4.42\n",
      "Average terminal state for agent 2:  21.58 \n",
      "\n",
      "Average step to solve epoch 147:  7.16\n",
      "Average reward per episode in epoch 147:  -9.25\n",
      "Average terminal state for agent 1:  4.64\n",
      "Average terminal state for agent 2:  21.36 \n",
      "\n",
      "Average step to solve epoch 148:  7.25\n",
      "Average reward per episode in epoch 148:  -10.29\n",
      "Average terminal state for agent 1:  5.52\n",
      "Average terminal state for agent 2:  20.48 \n",
      "\n",
      "Average step to solve epoch 149:  7.22\n",
      "Average reward per episode in epoch 149:  -9.785\n",
      "Average terminal state for agent 1:  5.52\n",
      "Average terminal state for agent 2:  20.48 \n",
      "\n",
      "Average step to solve epoch 150:  7.31\n",
      "Average reward per episode in epoch 150:  -9.59\n",
      "Average terminal state for agent 1:  5.52\n",
      "Average terminal state for agent 2:  20.48 \n",
      "\n",
      "Average step to solve epoch 151:  7.09\n",
      "Average reward per episode in epoch 151:  -9.655\n",
      "Average terminal state for agent 1:  5.3\n",
      "Average terminal state for agent 2:  20.7 \n",
      "\n",
      "Average step to solve epoch 152:  6.91\n",
      "Average reward per episode in epoch 152:  -8.05\n",
      "Average terminal state for agent 1:  5.74\n",
      "Average terminal state for agent 2:  20.26 \n",
      "\n",
      "Average step to solve epoch 153:  7.16\n",
      "Average reward per episode in epoch 153:  -9.155\n",
      "Average terminal state for agent 1:  5.52\n",
      "Average terminal state for agent 2:  20.48 \n",
      "\n",
      "Average step to solve epoch 154:  6.82\n",
      "Average reward per episode in epoch 154:  -9.48\n",
      "Average terminal state for agent 1:  3.98\n",
      "Average terminal state for agent 2:  22.02 \n",
      "\n",
      "Average step to solve epoch 155:  6.95\n",
      "Average reward per episode in epoch 155:  -9.135\n",
      "Average terminal state for agent 1:  4.64\n",
      "Average terminal state for agent 2:  21.36 \n",
      "\n",
      "Average step to solve epoch 156:  6.91\n",
      "Average reward per episode in epoch 156:  -8.525\n",
      "Average terminal state for agent 1:  4.42\n",
      "Average terminal state for agent 2:  21.58 \n",
      "\n",
      "Average step to solve epoch 157:  7.15\n",
      "Average reward per episode in epoch 157:  -10.0\n",
      "Average terminal state for agent 1:  4.2\n",
      "Average terminal state for agent 2:  21.8 \n",
      "\n",
      "Average step to solve epoch 158:  7.09\n",
      "Average reward per episode in epoch 158:  -9.56\n",
      "Average terminal state for agent 1:  4.64\n",
      "Average terminal state for agent 2:  21.36 \n",
      "\n",
      "Average step to solve epoch 159:  6.79\n",
      "Average reward per episode in epoch 159:  -8.215\n",
      "Average terminal state for agent 1:  3.76\n",
      "Average terminal state for agent 2:  22.24 \n",
      "\n",
      "Average step to solve epoch 160:  7.29\n",
      "Average reward per episode in epoch 160:  -9.38\n",
      "Average terminal state for agent 1:  4.64\n",
      "Average terminal state for agent 2:  21.36 \n",
      "\n",
      "Average step to solve epoch 161:  7.17\n",
      "Average reward per episode in epoch 161:  -9.355\n",
      "Average terminal state for agent 1:  5.52\n",
      "Average terminal state for agent 2:  20.48 \n",
      "\n",
      "Average step to solve epoch 162:  7.01\n",
      "Average reward per episode in epoch 162:  -8.72\n",
      "Average terminal state for agent 1:  4.42\n",
      "Average terminal state for agent 2:  21.58 \n",
      "\n",
      "Average step to solve epoch 163:  7.06\n",
      "Average reward per episode in epoch 163:  -8.77\n",
      "Average terminal state for agent 1:  5.3\n",
      "Average terminal state for agent 2:  20.7 \n",
      "\n",
      "Average step to solve epoch 164:  7.14\n",
      "Average reward per episode in epoch 164:  -8.85\n",
      "Average terminal state for agent 1:  6.18\n",
      "Average terminal state for agent 2:  19.82 \n",
      "\n",
      "Average step to solve epoch 165:  7.01\n",
      "Average reward per episode in epoch 165:  -8.435\n",
      "Average terminal state for agent 1:  5.74\n",
      "Average terminal state for agent 2:  20.26 \n",
      "\n",
      "Average step to solve epoch 166:  7.22\n",
      "Average reward per episode in epoch 166:  -9.12\n",
      "Average terminal state for agent 1:  5.08\n",
      "Average terminal state for agent 2:  20.92 \n",
      "\n",
      "Average step to solve epoch 167:  6.92\n",
      "Average reward per episode in epoch 167:  -9.105\n",
      "Average terminal state for agent 1:  4.64\n",
      "Average terminal state for agent 2:  21.36 \n",
      "\n",
      "Average step to solve epoch 168:  6.9\n",
      "Average reward per episode in epoch 168:  -8.99\n",
      "Average terminal state for agent 1:  4.2\n",
      "Average terminal state for agent 2:  21.8 \n",
      "\n",
      "Average step to solve epoch 169:  6.77\n",
      "Average reward per episode in epoch 169:  -8.575\n",
      "Average terminal state for agent 1:  3.76\n",
      "Average terminal state for agent 2:  22.24 \n",
      "\n",
      "Average step to solve epoch 170:  6.75\n",
      "Average reward per episode in epoch 170:  -8.555\n",
      "Average terminal state for agent 1:  3.54\n",
      "Average terminal state for agent 2:  22.46 \n",
      "\n",
      "Average step to solve epoch 171:  6.75\n",
      "Average reward per episode in epoch 171:  -8.365\n",
      "Average terminal state for agent 1:  4.42\n",
      "Average terminal state for agent 2:  21.58 \n",
      "\n",
      "Average step to solve epoch 172:  6.63\n",
      "Average reward per episode in epoch 172:  -8.15\n",
      "Average terminal state for agent 1:  4.2\n",
      "Average terminal state for agent 2:  21.8 \n",
      "\n",
      "Average step to solve epoch 173:  6.61\n",
      "Average reward per episode in epoch 173:  -8.035\n",
      "Average terminal state for agent 1:  4.2\n",
      "Average terminal state for agent 2:  21.8 \n",
      "\n",
      "Average step to solve epoch 174:  6.84\n",
      "Average reward per episode in epoch 174:  -8.075\n",
      "Average terminal state for agent 1:  4.2\n",
      "Average terminal state for agent 2:  21.8 \n",
      "\n",
      "Average step to solve epoch 175:  6.8\n",
      "Average reward per episode in epoch 175:  -8.89\n",
      "Average terminal state for agent 1:  4.42\n",
      "Average terminal state for agent 2:  21.58 \n",
      "\n",
      "Average step to solve epoch 176:  6.88\n",
      "Average reward per episode in epoch 176:  -8.495\n",
      "Average terminal state for agent 1:  4.42\n",
      "Average terminal state for agent 2:  21.58 \n",
      "\n",
      "Average step to solve epoch 177:  6.69\n",
      "Average reward per episode in epoch 177:  -8.59\n",
      "Average terminal state for agent 1:  4.42\n",
      "Average terminal state for agent 2:  21.58 \n",
      "\n",
      "Average step to solve epoch 178:  6.99\n",
      "Average reward per episode in epoch 178:  -8.605\n",
      "Average terminal state for agent 1:  4.64\n",
      "Average terminal state for agent 2:  21.36 \n",
      "\n",
      "Average step to solve epoch 179:  6.96\n",
      "Average reward per episode in epoch 179:  -9.43\n",
      "Average terminal state for agent 1:  3.76\n",
      "Average terminal state for agent 2:  22.24 \n",
      "\n",
      "Average step to solve epoch 180:  6.8\n",
      "Average reward per episode in epoch 180:  -8.605\n",
      "Average terminal state for agent 1:  4.86\n",
      "Average terminal state for agent 2:  21.14 \n",
      "\n",
      "Average step to solve epoch 181:  6.99\n",
      "Average reward per episode in epoch 181:  -9.84\n",
      "Average terminal state for agent 1:  4.42\n",
      "Average terminal state for agent 2:  21.58 \n",
      "\n",
      "Average step to solve epoch 182:  6.87\n",
      "Average reward per episode in epoch 182:  -9.15\n",
      "Average terminal state for agent 1:  4.64\n",
      "Average terminal state for agent 2:  21.36 \n",
      "\n",
      "Average step to solve epoch 183:  6.7\n",
      "Average reward per episode in epoch 183:  -7.935\n",
      "Average terminal state for agent 1:  5.74\n",
      "Average terminal state for agent 2:  20.26 \n",
      "\n",
      "Average step to solve epoch 184:  6.68\n",
      "Average reward per episode in epoch 184:  -8.2\n",
      "Average terminal state for agent 1:  4.2\n",
      "Average terminal state for agent 2:  21.8 \n",
      "\n",
      "Average step to solve epoch 185:  6.88\n",
      "Average reward per episode in epoch 185:  -8.115\n",
      "Average terminal state for agent 1:  4.42\n",
      "Average terminal state for agent 2:  21.58 \n",
      "\n",
      "Average step to solve epoch 186:  6.83\n",
      "Average reward per episode in epoch 186:  -8.92\n",
      "Average terminal state for agent 1:  3.98\n",
      "Average terminal state for agent 2:  22.02 \n",
      "\n",
      "Average step to solve epoch 187:  6.69\n",
      "Average reward per episode in epoch 187:  -8.305\n",
      "Average terminal state for agent 1:  4.86\n",
      "Average terminal state for agent 2:  21.14 \n",
      "\n",
      "Average step to solve epoch 188:  6.63\n",
      "Average reward per episode in epoch 188:  -7.77\n",
      "Average terminal state for agent 1:  3.76\n",
      "Average terminal state for agent 2:  22.24 \n",
      "\n",
      "Average step to solve epoch 189:  6.96\n",
      "Average reward per episode in epoch 189:  -9.335\n",
      "Average terminal state for agent 1:  3.32\n",
      "Average terminal state for agent 2:  22.68 \n",
      "\n",
      "Average step to solve epoch 190:  6.85\n",
      "Average reward per episode in epoch 190:  -9.13\n",
      "Average terminal state for agent 1:  5.08\n",
      "Average terminal state for agent 2:  20.92 \n",
      "\n",
      "Average step to solve epoch 191:  6.83\n",
      "Average reward per episode in epoch 191:  -8.635\n",
      "Average terminal state for agent 1:  4.64\n",
      "Average terminal state for agent 2:  21.36 \n",
      "\n",
      "Average step to solve epoch 192:  6.75\n",
      "Average reward per episode in epoch 192:  -8.27\n",
      "Average terminal state for agent 1:  4.42\n",
      "Average terminal state for agent 2:  21.58 \n",
      "\n",
      "Average step to solve epoch 193:  6.62\n",
      "Average reward per episode in epoch 193:  -7.855\n",
      "Average terminal state for agent 1:  3.54\n",
      "Average terminal state for agent 2:  22.46 \n",
      "\n",
      "Average step to solve epoch 194:  6.77\n",
      "Average reward per episode in epoch 194:  -8.385\n",
      "Average terminal state for agent 1:  3.54\n",
      "Average terminal state for agent 2:  22.46 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average step to solve epoch 195:  6.84\n",
      "Average reward per episode in epoch 195:  -8.645\n",
      "Average terminal state for agent 1:  3.54\n",
      "Average terminal state for agent 2:  22.46 \n",
      "\n",
      "Average step to solve epoch 196:  6.7\n",
      "Average reward per episode in epoch 196:  -8.885\n",
      "Average terminal state for agent 1:  4.2\n",
      "Average terminal state for agent 2:  21.8 \n",
      "\n",
      "Average step to solve epoch 197:  6.7\n",
      "Average reward per episode in epoch 197:  -7.65\n",
      "Average terminal state for agent 1:  4.64\n",
      "Average terminal state for agent 2:  21.36 \n",
      "\n",
      "Average step to solve epoch 198:  6.76\n",
      "Average reward per episode in epoch 198:  -7.9\n",
      "Average terminal state for agent 1:  4.2\n",
      "Average terminal state for agent 2:  21.8 \n",
      "\n",
      "Average step to solve epoch 199:  6.71\n",
      "Average reward per episode in epoch 199:  -8.23\n",
      "Average terminal state for agent 1:  3.98\n",
      "Average terminal state for agent 2:  22.02 \n",
      "\n",
      "Average step to solve epoch 200:  6.62\n",
      "Average reward per episode in epoch 200:  -8.425\n",
      "Average terminal state for agent 1:  3.1\n",
      "Average terminal state for agent 2:  22.9 \n",
      "\n",
      "Average step to solve epoch 201:  6.65\n",
      "Average reward per episode in epoch 201:  -7.79\n",
      "Average terminal state for agent 1:  3.98\n",
      "Average terminal state for agent 2:  22.02 \n",
      "\n",
      "Average step to solve epoch 202:  6.69\n",
      "Average reward per episode in epoch 202:  -8.495\n",
      "Average terminal state for agent 1:  3.76\n",
      "Average terminal state for agent 2:  22.24 \n",
      "\n",
      "Average step to solve epoch 203:  6.65\n",
      "Average reward per episode in epoch 203:  -7.695\n",
      "Average terminal state for agent 1:  3.76\n",
      "Average terminal state for agent 2:  22.24 \n",
      "\n",
      "Average step to solve epoch 204:  6.54\n",
      "Average reward per episode in epoch 204:  -7.87\n",
      "Average terminal state for agent 1:  3.76\n",
      "Average terminal state for agent 2:  22.24 \n",
      "\n",
      "Average step to solve epoch 205:  6.58\n",
      "Average reward per episode in epoch 205:  -7.815\n",
      "Average terminal state for agent 1:  3.98\n",
      "Average terminal state for agent 2:  22.02 \n",
      "\n",
      "Average step to solve epoch 206:  6.58\n",
      "Average reward per episode in epoch 206:  -8.385\n",
      "Average terminal state for agent 1:  3.54\n",
      "Average terminal state for agent 2:  22.46 \n",
      "\n",
      "Average step to solve epoch 207:  6.42\n",
      "Average reward per episode in epoch 207:  -7.465\n",
      "Average terminal state for agent 1:  3.1\n",
      "Average terminal state for agent 2:  22.9 \n",
      "\n",
      "Average step to solve epoch 208:  6.73\n",
      "Average reward per episode in epoch 208:  -8.535\n",
      "Average terminal state for agent 1:  3.98\n",
      "Average terminal state for agent 2:  22.02 \n",
      "\n",
      "Average step to solve epoch 209:  6.54\n",
      "Average reward per episode in epoch 209:  -7.775\n",
      "Average terminal state for agent 1:  3.54\n",
      "Average terminal state for agent 2:  22.46 \n",
      "\n",
      "Average step to solve epoch 210:  6.48\n",
      "Average reward per episode in epoch 210:  -7.145\n",
      "Average terminal state for agent 1:  3.54\n",
      "Average terminal state for agent 2:  22.46 \n",
      "\n",
      "Average step to solve epoch 211:  6.44\n",
      "Average reward per episode in epoch 211:  -7.485\n",
      "Average terminal state for agent 1:  3.32\n",
      "Average terminal state for agent 2:  22.68 \n",
      "\n",
      "Average step to solve epoch 212:  6.7\n",
      "Average reward per episode in epoch 212:  -8.03\n",
      "Average terminal state for agent 1:  2.88\n",
      "Average terminal state for agent 2:  23.12 \n",
      "\n",
      "Average step to solve epoch 213:  6.54\n",
      "Average reward per episode in epoch 213:  -7.87\n",
      "Average terminal state for agent 1:  3.54\n",
      "Average terminal state for agent 2:  22.46 \n",
      "\n",
      "Average step to solve epoch 214:  6.44\n",
      "Average reward per episode in epoch 214:  -7.77\n",
      "Average terminal state for agent 1:  3.32\n",
      "Average terminal state for agent 2:  22.68 \n",
      "\n",
      "Average step to solve epoch 215:  6.59\n",
      "Average reward per episode in epoch 215:  -7.54\n",
      "Average terminal state for agent 1:  4.2\n",
      "Average terminal state for agent 2:  21.8 \n",
      "\n",
      "Average step to solve epoch 216:  6.7\n",
      "Average reward per episode in epoch 216:  -8.505\n",
      "Average terminal state for agent 1:  3.54\n",
      "Average terminal state for agent 2:  22.46 \n",
      "\n",
      "Average step to solve epoch 217:  6.59\n",
      "Average reward per episode in epoch 217:  -7.92\n",
      "Average terminal state for agent 1:  3.98\n",
      "Average terminal state for agent 2:  22.02 \n",
      "\n",
      "Average step to solve epoch 218:  6.49\n",
      "Average reward per episode in epoch 218:  -7.915\n",
      "Average terminal state for agent 1:  2.88\n",
      "Average terminal state for agent 2:  23.12 \n",
      "\n",
      "Average step to solve epoch 219:  6.51\n",
      "Average reward per episode in epoch 219:  -7.175\n",
      "Average terminal state for agent 1:  3.32\n",
      "Average terminal state for agent 2:  22.68 \n",
      "\n",
      "Average step to solve epoch 220:  6.45\n",
      "Average reward per episode in epoch 220:  -7.875\n",
      "Average terminal state for agent 1:  3.98\n",
      "Average terminal state for agent 2:  22.02 \n",
      "\n",
      "Average step to solve epoch 221:  6.7\n",
      "Average reward per episode in epoch 221:  -7.745\n",
      "Average terminal state for agent 1:  3.98\n",
      "Average terminal state for agent 2:  22.02 \n",
      "\n",
      "Average step to solve epoch 222:  6.44\n",
      "Average reward per episode in epoch 222:  -8.245\n",
      "Average terminal state for agent 1:  3.1\n",
      "Average terminal state for agent 2:  22.9 \n",
      "\n",
      "Average step to solve epoch 223:  6.51\n",
      "Average reward per episode in epoch 223:  -7.365\n",
      "Average terminal state for agent 1:  2.88\n",
      "Average terminal state for agent 2:  23.12 \n",
      "\n",
      "Average step to solve epoch 224:  6.49\n",
      "Average reward per episode in epoch 224:  -7.25\n",
      "Average terminal state for agent 1:  3.32\n",
      "Average terminal state for agent 2:  22.68 \n",
      "\n",
      "Average step to solve epoch 225:  6.54\n",
      "Average reward per episode in epoch 225:  -7.775\n",
      "Average terminal state for agent 1:  3.98\n",
      "Average terminal state for agent 2:  22.02 \n",
      "\n",
      "Average step to solve epoch 226:  6.53\n",
      "Average reward per episode in epoch 226:  -7.955\n",
      "Average terminal state for agent 1:  3.54\n",
      "Average terminal state for agent 2:  22.46 \n",
      "\n",
      "Average step to solve epoch 227:  6.45\n",
      "Average reward per episode in epoch 227:  -7.59\n",
      "Average terminal state for agent 1:  3.32\n",
      "Average terminal state for agent 2:  22.68 \n",
      "\n",
      "Average step to solve epoch 228:  6.39\n",
      "Average reward per episode in epoch 228:  -6.865\n",
      "Average terminal state for agent 1:  3.98\n",
      "Average terminal state for agent 2:  22.02 \n",
      "\n",
      "Average step to solve epoch 229:  6.33\n",
      "Average reward per episode in epoch 229:  -7.47\n",
      "Average terminal state for agent 1:  3.1\n",
      "Average terminal state for agent 2:  22.9 \n",
      "\n",
      "Average step to solve epoch 230:  6.42\n",
      "Average reward per episode in epoch 230:  -7.655\n",
      "Average terminal state for agent 1:  2.88\n",
      "Average terminal state for agent 2:  23.12 \n",
      "\n",
      "Average step to solve epoch 231:  6.56\n",
      "Average reward per episode in epoch 231:  -8.175\n",
      "Average terminal state for agent 1:  3.32\n",
      "Average terminal state for agent 2:  22.68 \n",
      "\n",
      "Average step to solve epoch 232:  6.46\n",
      "Average reward per episode in epoch 232:  -7.695\n",
      "Average terminal state for agent 1:  2.66\n",
      "Average terminal state for agent 2:  23.34 \n",
      "\n",
      "Average step to solve epoch 233:  6.67\n",
      "Average reward per episode in epoch 233:  -7.905\n",
      "Average terminal state for agent 1:  3.32\n",
      "Average terminal state for agent 2:  22.68 \n",
      "\n",
      "Average step to solve epoch 234:  6.4\n",
      "Average reward per episode in epoch 234:  -7.255\n",
      "Average terminal state for agent 1:  2.88\n",
      "Average terminal state for agent 2:  23.12 \n",
      "\n",
      "Average step to solve epoch 235:  6.41\n",
      "Average reward per episode in epoch 235:  -7.17\n",
      "Average terminal state for agent 1:  2.88\n",
      "Average terminal state for agent 2:  23.12 \n",
      "\n",
      "Average step to solve epoch 236:  6.36\n",
      "Average reward per episode in epoch 236:  -7.31\n",
      "Average terminal state for agent 1:  2.88\n",
      "Average terminal state for agent 2:  23.12 \n",
      "\n",
      "Average step to solve epoch 237:  6.5\n",
      "Average reward per episode in epoch 237:  -7.355\n",
      "Average terminal state for agent 1:  3.1\n",
      "Average terminal state for agent 2:  22.9 \n",
      "\n",
      "Average step to solve epoch 238:  6.54\n",
      "Average reward per episode in epoch 238:  -7.395\n",
      "Average terminal state for agent 1:  3.76\n",
      "Average terminal state for agent 2:  22.24 \n",
      "\n",
      "Average step to solve epoch 239:  6.52\n",
      "Average reward per episode in epoch 239:  -6.995\n",
      "Average terminal state for agent 1:  3.54\n",
      "Average terminal state for agent 2:  22.46 \n",
      "\n",
      "Average step to solve epoch 240:  6.42\n",
      "Average reward per episode in epoch 240:  -7.18\n",
      "Average terminal state for agent 1:  3.32\n",
      "Average terminal state for agent 2:  22.68 \n",
      "\n",
      "Average step to solve epoch 241:  6.64\n",
      "Average reward per episode in epoch 241:  -7.97\n",
      "Average terminal state for agent 1:  3.76\n",
      "Average terminal state for agent 2:  22.24 \n",
      "\n",
      "Average step to solve epoch 242:  6.39\n",
      "Average reward per episode in epoch 242:  -7.055\n",
      "Average terminal state for agent 1:  3.54\n",
      "Average terminal state for agent 2:  22.46 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average step to solve epoch 243:  6.34\n",
      "Average reward per episode in epoch 243:  -7.29\n",
      "Average terminal state for agent 1:  2.66\n",
      "Average terminal state for agent 2:  23.34 \n",
      "\n",
      "Average step to solve epoch 244:  6.22\n",
      "Average reward per episode in epoch 244:  -6.695\n",
      "Average terminal state for agent 1:  3.1\n",
      "Average terminal state for agent 2:  22.9 \n",
      "\n",
      "Average step to solve epoch 245:  6.37\n",
      "Average reward per episode in epoch 245:  -7.225\n",
      "Average terminal state for agent 1:  3.32\n",
      "Average terminal state for agent 2:  22.68 \n",
      "\n",
      "Average step to solve epoch 246:  6.4\n",
      "Average reward per episode in epoch 246:  -7.065\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 247:  6.35\n",
      "Average reward per episode in epoch 247:  -7.205\n",
      "Average terminal state for agent 1:  3.1\n",
      "Average terminal state for agent 2:  22.9 \n",
      "\n",
      "Average step to solve epoch 248:  6.28\n",
      "Average reward per episode in epoch 248:  -6.66\n",
      "Average terminal state for agent 1:  3.76\n",
      "Average terminal state for agent 2:  22.24 \n",
      "\n",
      "Average step to solve epoch 249:  6.35\n",
      "Average reward per episode in epoch 249:  -7.395\n",
      "Average terminal state for agent 1:  2.66\n",
      "Average terminal state for agent 2:  23.34 \n",
      "\n",
      "Average step to solve epoch 250:  6.29\n",
      "Average reward per episode in epoch 250:  -6.955\n",
      "Average terminal state for agent 1:  3.32\n",
      "Average terminal state for agent 2:  22.68 \n",
      "\n",
      "Average step to solve epoch 251:  6.41\n",
      "Average reward per episode in epoch 251:  -6.98\n",
      "Average terminal state for agent 1:  2.88\n",
      "Average terminal state for agent 2:  23.12 \n",
      "\n",
      "Average step to solve epoch 252:  6.33\n",
      "Average reward per episode in epoch 252:  -7.185\n",
      "Average terminal state for agent 1:  2.88\n",
      "Average terminal state for agent 2:  23.12 \n",
      "\n",
      "Average step to solve epoch 253:  6.43\n",
      "Average reward per episode in epoch 253:  -7.855\n",
      "Average terminal state for agent 1:  3.1\n",
      "Average terminal state for agent 2:  22.9 \n",
      "\n",
      "Average step to solve epoch 254:  6.34\n",
      "Average reward per episode in epoch 254:  -7.005\n",
      "Average terminal state for agent 1:  2.88\n",
      "Average terminal state for agent 2:  23.12 \n",
      "\n",
      "Average step to solve epoch 255:  6.32\n",
      "Average reward per episode in epoch 255:  -6.985\n",
      "Average terminal state for agent 1:  3.54\n",
      "Average terminal state for agent 2:  22.46 \n",
      "\n",
      "Average step to solve epoch 256:  6.28\n",
      "Average reward per episode in epoch 256:  -6.85\n",
      "Average terminal state for agent 1:  2.66\n",
      "Average terminal state for agent 2:  23.34 \n",
      "\n",
      "Average step to solve epoch 257:  6.3\n",
      "Average reward per episode in epoch 257:  -6.87\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 258:  6.29\n",
      "Average reward per episode in epoch 258:  -6.765\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 259:  6.36\n",
      "Average reward per episode in epoch 259:  -7.215\n",
      "Average terminal state for agent 1:  2.88\n",
      "Average terminal state for agent 2:  23.12 \n",
      "\n",
      "Average step to solve epoch 260:  6.38\n",
      "Average reward per episode in epoch 260:  -6.57\n",
      "Average terminal state for agent 1:  3.32\n",
      "Average terminal state for agent 2:  22.68 \n",
      "\n",
      "Average step to solve epoch 261:  6.4\n",
      "Average reward per episode in epoch 261:  -7.065\n",
      "Average terminal state for agent 1:  3.76\n",
      "Average terminal state for agent 2:  22.24 \n",
      "\n",
      "Average step to solve epoch 262:  6.47\n",
      "Average reward per episode in epoch 262:  -7.135\n",
      "Average terminal state for agent 1:  2.88\n",
      "Average terminal state for agent 2:  23.12 \n",
      "\n",
      "Average step to solve epoch 263:  6.28\n",
      "Average reward per episode in epoch 263:  -7.04\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 264:  6.37\n",
      "Average reward per episode in epoch 264:  -7.51\n",
      "Average terminal state for agent 1:  3.54\n",
      "Average terminal state for agent 2:  22.46 \n",
      "\n",
      "Average step to solve epoch 265:  6.28\n",
      "Average reward per episode in epoch 265:  -6.66\n",
      "Average terminal state for agent 1:  3.1\n",
      "Average terminal state for agent 2:  22.9 \n",
      "\n",
      "Average step to solve epoch 266:  6.32\n",
      "Average reward per episode in epoch 266:  -7.46\n",
      "Average terminal state for agent 1:  3.76\n",
      "Average terminal state for agent 2:  22.24 \n",
      "\n",
      "Average step to solve epoch 267:  6.29\n",
      "Average reward per episode in epoch 267:  -6.575\n",
      "Average terminal state for agent 1:  3.1\n",
      "Average terminal state for agent 2:  22.9 \n",
      "\n",
      "Average step to solve epoch 268:  6.35\n",
      "Average reward per episode in epoch 268:  -7.015\n",
      "Average terminal state for agent 1:  2.66\n",
      "Average terminal state for agent 2:  23.34 \n",
      "\n",
      "Average step to solve epoch 269:  6.25\n",
      "Average reward per episode in epoch 269:  -6.63\n",
      "Average terminal state for agent 1:  2.66\n",
      "Average terminal state for agent 2:  23.34 \n",
      "\n",
      "Average step to solve epoch 270:  6.26\n",
      "Average reward per episode in epoch 270:  -7.02\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 271:  6.55\n",
      "Average reward per episode in epoch 271:  -7.5\n",
      "Average terminal state for agent 1:  2.88\n",
      "Average terminal state for agent 2:  23.12 \n",
      "\n",
      "Average step to solve epoch 272:  6.51\n",
      "Average reward per episode in epoch 272:  -7.46\n",
      "Average terminal state for agent 1:  3.32\n",
      "Average terminal state for agent 2:  22.68 \n",
      "\n",
      "Average step to solve epoch 273:  6.29\n",
      "Average reward per episode in epoch 273:  -6.765\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 274:  6.31\n",
      "Average reward per episode in epoch 274:  -7.165\n",
      "Average terminal state for agent 1:  3.54\n",
      "Average terminal state for agent 2:  22.46 \n",
      "\n",
      "Average step to solve epoch 275:  6.24\n",
      "Average reward per episode in epoch 275:  -6.335\n",
      "Average terminal state for agent 1:  3.32\n",
      "Average terminal state for agent 2:  22.68 \n",
      "\n",
      "Average step to solve epoch 276:  6.33\n",
      "Average reward per episode in epoch 276:  -6.9\n",
      "Average terminal state for agent 1:  2.66\n",
      "Average terminal state for agent 2:  23.34 \n",
      "\n",
      "Average step to solve epoch 277:  6.28\n",
      "Average reward per episode in epoch 277:  -6.66\n",
      "Average terminal state for agent 1:  2.88\n",
      "Average terminal state for agent 2:  23.12 \n",
      "\n",
      "Average step to solve epoch 278:  6.54\n",
      "Average reward per episode in epoch 278:  -7.775\n",
      "Average terminal state for agent 1:  3.32\n",
      "Average terminal state for agent 2:  22.68 \n",
      "\n",
      "Average step to solve epoch 279:  6.43\n",
      "Average reward per episode in epoch 279:  -7.76\n",
      "Average terminal state for agent 1:  3.32\n",
      "Average terminal state for agent 2:  22.68 \n",
      "\n",
      "Average step to solve epoch 280:  6.43\n",
      "Average reward per episode in epoch 280:  -6.715\n",
      "Average terminal state for agent 1:  3.76\n",
      "Average terminal state for agent 2:  22.24 \n",
      "\n",
      "Average step to solve epoch 281:  6.36\n",
      "Average reward per episode in epoch 281:  -6.74\n",
      "Average terminal state for agent 1:  3.32\n",
      "Average terminal state for agent 2:  22.68 \n",
      "\n",
      "Average step to solve epoch 282:  6.31\n",
      "Average reward per episode in epoch 282:  -6.88\n",
      "Average terminal state for agent 1:  3.1\n",
      "Average terminal state for agent 2:  22.9 \n",
      "\n",
      "Average step to solve epoch 283:  6.3\n",
      "Average reward per episode in epoch 283:  -6.87\n",
      "Average terminal state for agent 1:  3.32\n",
      "Average terminal state for agent 2:  22.68 \n",
      "\n",
      "Average step to solve epoch 284:  6.41\n",
      "Average reward per episode in epoch 284:  -7.17\n",
      "Average terminal state for agent 1:  2.66\n",
      "Average terminal state for agent 2:  23.34 \n",
      "\n",
      "Average step to solve epoch 285:  6.16\n",
      "Average reward per episode in epoch 285:  -6.54\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 286:  6.17\n",
      "Average reward per episode in epoch 286:  -6.265\n",
      "Average terminal state for agent 1:  2.66\n",
      "Average terminal state for agent 2:  23.34 \n",
      "\n",
      "Average step to solve epoch 287:  6.28\n",
      "Average reward per episode in epoch 287:  -6.755\n",
      "Average terminal state for agent 1:  2.88\n",
      "Average terminal state for agent 2:  23.12 \n",
      "\n",
      "Average step to solve epoch 288:  6.22\n",
      "Average reward per episode in epoch 288:  -6.315\n",
      "Average terminal state for agent 1:  2.66\n",
      "Average terminal state for agent 2:  23.34 \n",
      "\n",
      "Average step to solve epoch 289:  6.31\n",
      "Average reward per episode in epoch 289:  -7.165\n",
      "Average terminal state for agent 1:  3.54\n",
      "Average terminal state for agent 2:  22.46 \n",
      "\n",
      "Average step to solve epoch 290:  6.28\n",
      "Average reward per episode in epoch 290:  -6.755\n",
      "Average terminal state for agent 1:  3.1\n",
      "Average terminal state for agent 2:  22.9 \n",
      "\n",
      "Average step to solve epoch 291:  6.12\n",
      "Average reward per episode in epoch 291:  -6.405\n",
      "Average terminal state for agent 1:  2.66\n",
      "Average terminal state for agent 2:  23.34 \n",
      "\n",
      "Average step to solve epoch 292:  6.22\n",
      "Average reward per episode in epoch 292:  -6.6\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average step to solve epoch 293:  6.18\n",
      "Average reward per episode in epoch 293:  -6.75\n",
      "Average terminal state for agent 1:  2.88\n",
      "Average terminal state for agent 2:  23.12 \n",
      "\n",
      "Average step to solve epoch 294:  6.13\n",
      "Average reward per episode in epoch 294:  -6.32\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 295:  6.25\n",
      "Average reward per episode in epoch 295:  -6.535\n",
      "Average terminal state for agent 1:  2.88\n",
      "Average terminal state for agent 2:  23.12 \n",
      "\n",
      "Average step to solve epoch 296:  6.19\n",
      "Average reward per episode in epoch 296:  -6.475\n",
      "Average terminal state for agent 1:  2.66\n",
      "Average terminal state for agent 2:  23.34 \n",
      "\n",
      "Average step to solve epoch 297:  6.16\n",
      "Average reward per episode in epoch 297:  -6.635\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 298:  6.21\n",
      "Average reward per episode in epoch 298:  -6.59\n",
      "Average terminal state for agent 1:  2.0\n",
      "Average terminal state for agent 2:  24.0 \n",
      "\n",
      "Average step to solve epoch 299:  6.22\n",
      "Average reward per episode in epoch 299:  -6.695\n",
      "Average terminal state for agent 1:  2.88\n",
      "Average terminal state for agent 2:  23.12 \n",
      "\n",
      "Average step to solve epoch 300:  6.2\n",
      "Average reward per episode in epoch 300:  -6.485\n",
      "Average terminal state for agent 1:  2.66\n",
      "Average terminal state for agent 2:  23.34 \n",
      "\n",
      "Average step to solve epoch 301:  6.18\n",
      "Average reward per episode in epoch 301:  -6.465\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 302:  6.21\n",
      "Average reward per episode in epoch 302:  -6.59\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 303:  6.28\n",
      "Average reward per episode in epoch 303:  -6.565\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 304:  6.18\n",
      "Average reward per episode in epoch 304:  -6.56\n",
      "Average terminal state for agent 1:  2.88\n",
      "Average terminal state for agent 2:  23.12 \n",
      "\n",
      "Average step to solve epoch 305:  6.23\n",
      "Average reward per episode in epoch 305:  -6.61\n",
      "Average terminal state for agent 1:  2.88\n",
      "Average terminal state for agent 2:  23.12 \n",
      "\n",
      "Average step to solve epoch 306:  6.19\n",
      "Average reward per episode in epoch 306:  -6.665\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 307:  6.15\n",
      "Average reward per episode in epoch 307:  -6.34\n",
      "Average terminal state for agent 1:  2.66\n",
      "Average terminal state for agent 2:  23.34 \n",
      "\n",
      "Average step to solve epoch 308:  6.25\n",
      "Average reward per episode in epoch 308:  -6.535\n",
      "Average terminal state for agent 1:  2.88\n",
      "Average terminal state for agent 2:  23.12 \n",
      "\n",
      "Average step to solve epoch 309:  6.23\n",
      "Average reward per episode in epoch 309:  -6.61\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 310:  6.29\n",
      "Average reward per episode in epoch 310:  -7.05\n",
      "Average terminal state for agent 1:  3.1\n",
      "Average terminal state for agent 2:  22.9 \n",
      "\n",
      "Average step to solve epoch 311:  6.25\n",
      "Average reward per episode in epoch 311:  -6.82\n",
      "Average terminal state for agent 1:  2.66\n",
      "Average terminal state for agent 2:  23.34 \n",
      "\n",
      "Average step to solve epoch 312:  6.24\n",
      "Average reward per episode in epoch 312:  -6.715\n",
      "Average terminal state for agent 1:  2.66\n",
      "Average terminal state for agent 2:  23.34 \n",
      "\n",
      "Average step to solve epoch 313:  6.14\n",
      "Average reward per episode in epoch 313:  -6.615\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 314:  6.21\n",
      "Average reward per episode in epoch 314:  -6.305\n",
      "Average terminal state for agent 1:  2.66\n",
      "Average terminal state for agent 2:  23.34 \n",
      "\n",
      "Average step to solve epoch 315:  6.23\n",
      "Average reward per episode in epoch 315:  -6.42\n",
      "Average terminal state for agent 1:  3.54\n",
      "Average terminal state for agent 2:  22.46 \n",
      "\n",
      "Average step to solve epoch 316:  6.15\n",
      "Average reward per episode in epoch 316:  -6.53\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 317:  6.21\n",
      "Average reward per episode in epoch 317:  -6.59\n",
      "Average terminal state for agent 1:  2.88\n",
      "Average terminal state for agent 2:  23.12 \n",
      "\n",
      "Average step to solve epoch 318:  6.12\n",
      "Average reward per episode in epoch 318:  -6.215\n",
      "Average terminal state for agent 1:  2.0\n",
      "Average terminal state for agent 2:  24.0 \n",
      "\n",
      "Average step to solve epoch 319:  6.16\n",
      "Average reward per episode in epoch 319:  -6.635\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 320:  6.13\n",
      "Average reward per episode in epoch 320:  -6.32\n",
      "Average terminal state for agent 1:  2.66\n",
      "Average terminal state for agent 2:  23.34 \n",
      "\n",
      "Average step to solve epoch 321:  6.08\n",
      "Average reward per episode in epoch 321:  -6.27\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 322:  6.2\n",
      "Average reward per episode in epoch 322:  -6.96\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 323:  6.3\n",
      "Average reward per episode in epoch 323:  -7.155\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 324:  6.23\n",
      "Average reward per episode in epoch 324:  -6.99\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 325:  6.12\n",
      "Average reward per episode in epoch 325:  -6.5\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 326:  6.11\n",
      "Average reward per episode in epoch 326:  -6.585\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 327:  6.31\n",
      "Average reward per episode in epoch 327:  -7.26\n",
      "Average terminal state for agent 1:  3.1\n",
      "Average terminal state for agent 2:  22.9 \n",
      "\n",
      "Average step to solve epoch 328:  6.17\n",
      "Average reward per episode in epoch 328:  -6.455\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 329:  6.09\n",
      "Average reward per episode in epoch 329:  -6.28\n",
      "Average terminal state for agent 1:  2.0\n",
      "Average terminal state for agent 2:  24.0 \n",
      "\n",
      "Average step to solve epoch 330:  6.29\n",
      "Average reward per episode in epoch 330:  -6.86\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 331:  6.13\n",
      "Average reward per episode in epoch 331:  -6.32\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 332:  6.13\n",
      "Average reward per episode in epoch 332:  -6.13\n",
      "Average terminal state for agent 1:  2.0\n",
      "Average terminal state for agent 2:  24.0 \n",
      "\n",
      "Average step to solve epoch 333:  6.12\n",
      "Average reward per episode in epoch 333:  -6.215\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 334:  6.18\n",
      "Average reward per episode in epoch 334:  -6.56\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 335:  6.14\n",
      "Average reward per episode in epoch 335:  -6.33\n",
      "Average terminal state for agent 1:  2.66\n",
      "Average terminal state for agent 2:  23.34 \n",
      "\n",
      "Average step to solve epoch 336:  6.09\n",
      "Average reward per episode in epoch 336:  -6.185\n",
      "Average terminal state for agent 1:  2.0\n",
      "Average terminal state for agent 2:  24.0 \n",
      "\n",
      "Average step to solve epoch 337:  6.25\n",
      "Average reward per episode in epoch 337:  -6.725\n",
      "Average terminal state for agent 1:  3.1\n",
      "Average terminal state for agent 2:  22.9 \n",
      "\n",
      "Average step to solve epoch 338:  6.16\n",
      "Average reward per episode in epoch 338:  -6.635\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 339:  6.14\n",
      "Average reward per episode in epoch 339:  -6.235\n",
      "Average terminal state for agent 1:  2.88\n",
      "Average terminal state for agent 2:  23.12 \n",
      "\n",
      "Average step to solve epoch 340:  6.23\n",
      "Average reward per episode in epoch 340:  -6.705\n",
      "Average terminal state for agent 1:  2.88\n",
      "Average terminal state for agent 2:  23.12 \n",
      "\n",
      "Average step to solve epoch 341:  6.12\n",
      "Average reward per episode in epoch 341:  -6.405\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average step to solve epoch 342:  6.16\n",
      "Average reward per episode in epoch 342:  -6.445\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 343:  6.19\n",
      "Average reward per episode in epoch 343:  -6.38\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 344:  6.22\n",
      "Average reward per episode in epoch 344:  -6.41\n",
      "Average terminal state for agent 1:  2.66\n",
      "Average terminal state for agent 2:  23.34 \n",
      "\n",
      "Average step to solve epoch 345:  6.19\n",
      "Average reward per episode in epoch 345:  -6.285\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 346:  6.16\n",
      "Average reward per episode in epoch 346:  -6.73\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 347:  6.18\n",
      "Average reward per episode in epoch 347:  -6.56\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 348:  6.13\n",
      "Average reward per episode in epoch 348:  -6.51\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 349:  6.18\n",
      "Average reward per episode in epoch 349:  -6.37\n",
      "Average terminal state for agent 1:  2.66\n",
      "Average terminal state for agent 2:  23.34 \n",
      "\n",
      "Average step to solve epoch 350:  6.12\n",
      "Average reward per episode in epoch 350:  -6.69\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 351:  6.07\n",
      "Average reward per episode in epoch 351:  -6.165\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 352:  6.16\n",
      "Average reward per episode in epoch 352:  -6.35\n",
      "Average terminal state for agent 1:  2.0\n",
      "Average terminal state for agent 2:  24.0 \n",
      "\n",
      "Average step to solve epoch 353:  6.27\n",
      "Average reward per episode in epoch 353:  -6.745\n",
      "Average terminal state for agent 1:  3.76\n",
      "Average terminal state for agent 2:  22.24 \n",
      "\n",
      "Average step to solve epoch 354:  6.17\n",
      "Average reward per episode in epoch 354:  -6.645\n",
      "Average terminal state for agent 1:  2.0\n",
      "Average terminal state for agent 2:  24.0 \n",
      "\n",
      "Average step to solve epoch 355:  6.06\n",
      "Average reward per episode in epoch 355:  -6.25\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 356:  6.11\n",
      "Average reward per episode in epoch 356:  -6.3\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 357:  6.15\n",
      "Average reward per episode in epoch 357:  -6.435\n",
      "Average terminal state for agent 1:  2.66\n",
      "Average terminal state for agent 2:  23.34 \n",
      "\n",
      "Average step to solve epoch 358:  6.09\n",
      "Average reward per episode in epoch 358:  -6.47\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 359:  6.08\n",
      "Average reward per episode in epoch 359:  -6.365\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 360:  6.06\n",
      "Average reward per episode in epoch 360:  -6.155\n",
      "Average terminal state for agent 1:  2.0\n",
      "Average terminal state for agent 2:  24.0 \n",
      "\n",
      "Average step to solve epoch 361:  6.18\n",
      "Average reward per episode in epoch 361:  -6.37\n",
      "Average terminal state for agent 1:  2.66\n",
      "Average terminal state for agent 2:  23.34 \n",
      "\n",
      "Average step to solve epoch 362:  6.16\n",
      "Average reward per episode in epoch 362:  -6.255\n",
      "Average terminal state for agent 1:  2.66\n",
      "Average terminal state for agent 2:  23.34 \n",
      "\n",
      "Average step to solve epoch 363:  6.13\n",
      "Average reward per episode in epoch 363:  -6.415\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 364:  6.16\n",
      "Average reward per episode in epoch 364:  -6.635\n",
      "Average terminal state for agent 1:  2.66\n",
      "Average terminal state for agent 2:  23.34 \n",
      "\n",
      "Average step to solve epoch 365:  6.11\n",
      "Average reward per episode in epoch 365:  -6.395\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 366:  6.07\n",
      "Average reward per episode in epoch 366:  -6.355\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 367:  6.07\n",
      "Average reward per episode in epoch 367:  -6.165\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 368:  6.07\n",
      "Average reward per episode in epoch 368:  -6.165\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 369:  6.08\n",
      "Average reward per episode in epoch 369:  -6.175\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 370:  6.09\n",
      "Average reward per episode in epoch 370:  -6.09\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 371:  6.17\n",
      "Average reward per episode in epoch 371:  -6.74\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 372:  6.1\n",
      "Average reward per episode in epoch 372:  -6.29\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 373:  6.16\n",
      "Average reward per episode in epoch 373:  -6.635\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 374:  6.18\n",
      "Average reward per episode in epoch 374:  -6.275\n",
      "Average terminal state for agent 1:  2.66\n",
      "Average terminal state for agent 2:  23.34 \n",
      "\n",
      "Average step to solve epoch 375:  6.07\n",
      "Average reward per episode in epoch 375:  -6.26\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 376:  6.16\n",
      "Average reward per episode in epoch 376:  -6.35\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 377:  6.08\n",
      "Average reward per episode in epoch 377:  -6.365\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 378:  6.04\n",
      "Average reward per episode in epoch 378:  -6.04\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 379:  6.12\n",
      "Average reward per episode in epoch 379:  -6.5\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 380:  6.07\n",
      "Average reward per episode in epoch 380:  -6.26\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 381:  6.09\n",
      "Average reward per episode in epoch 381:  -6.375\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 382:  6.09\n",
      "Average reward per episode in epoch 382:  -6.185\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 383:  6.16\n",
      "Average reward per episode in epoch 383:  -6.54\n",
      "Average terminal state for agent 1:  2.66\n",
      "Average terminal state for agent 2:  23.34 \n",
      "\n",
      "Average step to solve epoch 384:  6.1\n",
      "Average reward per episode in epoch 384:  -6.29\n",
      "Average terminal state for agent 1:  2.0\n",
      "Average terminal state for agent 2:  24.0 \n",
      "\n",
      "Average step to solve epoch 385:  6.13\n",
      "Average reward per episode in epoch 385:  -6.32\n",
      "Average terminal state for agent 1:  2.66\n",
      "Average terminal state for agent 2:  23.34 \n",
      "\n",
      "Average step to solve epoch 386:  6.06\n",
      "Average reward per episode in epoch 386:  -6.155\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 387:  6.21\n",
      "Average reward per episode in epoch 387:  -6.78\n",
      "Average terminal state for agent 1:  2.88\n",
      "Average terminal state for agent 2:  23.12 \n",
      "\n",
      "Average step to solve epoch 388:  6.07\n",
      "Average reward per episode in epoch 388:  -6.26\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 389:  6.06\n",
      "Average reward per episode in epoch 389:  -6.25\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 390:  6.09\n",
      "Average reward per episode in epoch 390:  -6.28\n",
      "Average terminal state for agent 1:  2.88\n",
      "Average terminal state for agent 2:  23.12 \n",
      "\n",
      "Average step to solve epoch 391:  6.07\n",
      "Average reward per episode in epoch 391:  -6.07\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 392:  6.12\n",
      "Average reward per episode in epoch 392:  -6.405\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 393:  6.06\n",
      "Average reward per episode in epoch 393:  -6.25\n",
      "Average terminal state for agent 1:  2.0\n",
      "Average terminal state for agent 2:  24.0 \n",
      "\n",
      "Average step to solve epoch 394:  6.05\n",
      "Average reward per episode in epoch 394:  -6.05\n",
      "Average terminal state for agent 1:  2.0\n",
      "Average terminal state for agent 2:  24.0 \n",
      "\n",
      "Average step to solve epoch 395:  6.14\n",
      "Average reward per episode in epoch 395:  -6.52\n",
      "Average terminal state for agent 1:  2.0\n",
      "Average terminal state for agent 2:  24.0 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average step to solve epoch 396:  6.17\n",
      "Average reward per episode in epoch 396:  -6.455\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 397:  6.04\n",
      "Average reward per episode in epoch 397:  -6.135\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 398:  6.11\n",
      "Average reward per episode in epoch 398:  -6.3\n",
      "Average terminal state for agent 1:  2.0\n",
      "Average terminal state for agent 2:  24.0 \n",
      "\n",
      "Average step to solve epoch 399:  6.09\n",
      "Average reward per episode in epoch 399:  -6.375\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 400:  6.05\n",
      "Average reward per episode in epoch 400:  -6.145\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 401:  6.02\n",
      "Average reward per episode in epoch 401:  -6.115\n",
      "Average terminal state for agent 1:  2.0\n",
      "Average terminal state for agent 2:  24.0 \n",
      "\n",
      "Average step to solve epoch 402:  6.11\n",
      "Average reward per episode in epoch 402:  -6.3\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 403:  6.05\n",
      "Average reward per episode in epoch 403:  -6.24\n",
      "Average terminal state for agent 1:  2.0\n",
      "Average terminal state for agent 2:  24.0 \n",
      "\n",
      "Average step to solve epoch 404:  6.07\n",
      "Average reward per episode in epoch 404:  -6.07\n",
      "Average terminal state for agent 1:  2.0\n",
      "Average terminal state for agent 2:  24.0 \n",
      "\n",
      "Average step to solve epoch 405:  6.1\n",
      "Average reward per episode in epoch 405:  -6.48\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 406:  6.1\n",
      "Average reward per episode in epoch 406:  -6.385\n",
      "Average terminal state for agent 1:  2.0\n",
      "Average terminal state for agent 2:  24.0 \n",
      "\n",
      "Average step to solve epoch 407:  6.04\n",
      "Average reward per episode in epoch 407:  -6.04\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 408:  6.08\n",
      "Average reward per episode in epoch 408:  -6.27\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 409:  6.04\n",
      "Average reward per episode in epoch 409:  -6.135\n",
      "Average terminal state for agent 1:  2.0\n",
      "Average terminal state for agent 2:  24.0 \n",
      "\n",
      "Average step to solve epoch 410:  6.05\n",
      "Average reward per episode in epoch 410:  -6.24\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 411:  6.04\n",
      "Average reward per episode in epoch 411:  -6.135\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 412:  6.04\n",
      "Average reward per episode in epoch 412:  -6.135\n",
      "Average terminal state for agent 1:  2.0\n",
      "Average terminal state for agent 2:  24.0 \n",
      "\n",
      "Average step to solve epoch 413:  6.07\n",
      "Average reward per episode in epoch 413:  -6.26\n",
      "Average terminal state for agent 1:  2.0\n",
      "Average terminal state for agent 2:  24.0 \n",
      "\n",
      "Average step to solve epoch 414:  6.06\n",
      "Average reward per episode in epoch 414:  -6.345\n",
      "Average terminal state for agent 1:  2.0\n",
      "Average terminal state for agent 2:  24.0 \n",
      "\n",
      "Average step to solve epoch 415:  6.08\n",
      "Average reward per episode in epoch 415:  -6.27\n",
      "Average terminal state for agent 1:  2.0\n",
      "Average terminal state for agent 2:  24.0 \n",
      "\n",
      "Average step to solve epoch 416:  6.04\n",
      "Average reward per episode in epoch 416:  -6.135\n",
      "Average terminal state for agent 1:  2.0\n",
      "Average terminal state for agent 2:  24.0 \n",
      "\n",
      "Average step to solve epoch 417:  6.06\n",
      "Average reward per episode in epoch 417:  -6.345\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 418:  6.06\n",
      "Average reward per episode in epoch 418:  -6.155\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 419:  6.04\n",
      "Average reward per episode in epoch 419:  -6.135\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 420:  6.08\n",
      "Average reward per episode in epoch 420:  -6.27\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 421:  6.04\n",
      "Average reward per episode in epoch 421:  -6.135\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 422:  6.07\n",
      "Average reward per episode in epoch 422:  -6.355\n",
      "Average terminal state for agent 1:  2.0\n",
      "Average terminal state for agent 2:  24.0 \n",
      "\n",
      "Average step to solve epoch 423:  6.05\n",
      "Average reward per episode in epoch 423:  -6.145\n",
      "Average terminal state for agent 1:  2.0\n",
      "Average terminal state for agent 2:  24.0 \n",
      "\n",
      "Average step to solve epoch 424:  6.09\n",
      "Average reward per episode in epoch 424:  -6.09\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 425:  6.08\n",
      "Average reward per episode in epoch 425:  -6.08\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 426:  6.05\n",
      "Average reward per episode in epoch 426:  -6.145\n",
      "Average terminal state for agent 1:  2.0\n",
      "Average terminal state for agent 2:  24.0 \n",
      "\n",
      "Average step to solve epoch 427:  6.02\n",
      "Average reward per episode in epoch 427:  -6.115\n",
      "Average terminal state for agent 1:  2.0\n",
      "Average terminal state for agent 2:  24.0 \n",
      "\n",
      "Average step to solve epoch 428:  6.04\n",
      "Average reward per episode in epoch 428:  -6.04\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 429:  6.05\n",
      "Average reward per episode in epoch 429:  -6.335\n",
      "Average terminal state for agent 1:  2.0\n",
      "Average terminal state for agent 2:  24.0 \n",
      "\n",
      "Average step to solve epoch 430:  6.08\n",
      "Average reward per episode in epoch 430:  -6.175\n",
      "Average terminal state for agent 1:  2.0\n",
      "Average terminal state for agent 2:  24.0 \n",
      "\n",
      "Average step to solve epoch 431:  6.13\n",
      "Average reward per episode in epoch 431:  -6.415\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 432:  6.08\n",
      "Average reward per episode in epoch 432:  -6.365\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 433:  6.13\n",
      "Average reward per episode in epoch 433:  -6.32\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 434:  6.11\n",
      "Average reward per episode in epoch 434:  -6.3\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 435:  6.09\n",
      "Average reward per episode in epoch 435:  -6.375\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 436:  6.04\n",
      "Average reward per episode in epoch 436:  -6.135\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 437:  6.16\n",
      "Average reward per episode in epoch 437:  -6.35\n",
      "Average terminal state for agent 1:  2.66\n",
      "Average terminal state for agent 2:  23.34 \n",
      "\n",
      "Average step to solve epoch 438:  6.1\n",
      "Average reward per episode in epoch 438:  -6.29\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 439:  6.11\n",
      "Average reward per episode in epoch 439:  -6.49\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 440:  6.06\n",
      "Average reward per episode in epoch 440:  -6.25\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 441:  6.05\n",
      "Average reward per episode in epoch 441:  -6.24\n",
      "Average terminal state for agent 1:  2.0\n",
      "Average terminal state for agent 2:  24.0 \n",
      "\n",
      "Average step to solve epoch 442:  6.09\n",
      "Average reward per episode in epoch 442:  -6.28\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 443:  6.02\n",
      "Average reward per episode in epoch 443:  -6.115\n",
      "Average terminal state for agent 1:  2.0\n",
      "Average terminal state for agent 2:  24.0 \n",
      "\n",
      "Average step to solve epoch 444:  6.03\n",
      "Average reward per episode in epoch 444:  -6.03\n",
      "Average terminal state for agent 1:  2.0\n",
      "Average terminal state for agent 2:  24.0 \n",
      "\n",
      "Average step to solve epoch 445:  6.07\n",
      "Average reward per episode in epoch 445:  -6.07\n",
      "Average terminal state for agent 1:  2.0\n",
      "Average terminal state for agent 2:  24.0 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average step to solve epoch 446:  6.04\n",
      "Average reward per episode in epoch 446:  -6.04\n",
      "Average terminal state for agent 1:  2.0\n",
      "Average terminal state for agent 2:  24.0 \n",
      "\n",
      "Average step to solve epoch 447:  6.04\n",
      "Average reward per episode in epoch 447:  -6.04\n",
      "Average terminal state for agent 1:  2.44\n",
      "Average terminal state for agent 2:  23.56 \n",
      "\n",
      "Average step to solve epoch 448:  6.02\n",
      "Average reward per episode in epoch 448:  -6.02\n",
      "Average terminal state for agent 1:  2.0\n",
      "Average terminal state for agent 2:  24.0 \n",
      "\n",
      "Average step to solve epoch 449:  6.08\n",
      "Average reward per episode in epoch 449:  -6.08\n",
      "Average terminal state for agent 1:  2.0\n",
      "Average terminal state for agent 2:  24.0 \n",
      "\n",
      "Average step to solve epoch 450:  6.07\n",
      "Average reward per episode in epoch 450:  -6.165\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 451:  6.08\n",
      "Average reward per episode in epoch 451:  -6.27\n",
      "Average terminal state for agent 1:  2.22\n",
      "Average terminal state for agent 2:  23.78 \n",
      "\n",
      "Average step to solve epoch 452:  6.0\n",
      "Average reward per episode in epoch 452:  -6.0\n",
      "Average terminal state for agent 1:  2.0\n",
      "Average terminal state for agent 2:  24.0 \n",
      "\n",
      "Hey champion, you solve it!\n"
     ]
    }
   ],
   "source": [
    "# inizialization of the system\n",
    "grid_size = (5,5) # the environment at the moment works only for grid = 4x4\n",
    "n_actors = 2\n",
    "n_actions = 4**n_actors # cartesian product of the number of actions a1 x a2\n",
    "\n",
    "# notice that for a fortunate coincidence the terminal states are along the border of the map, thus trying to\n",
    "# go out of the map from those positions equals to staying still. This is good, because with two actors one could\n",
    "# have to wait the other; in that case the best action would be to remain in the terminal state, but it is not provided \n",
    "# directly of that action. (one can of course implement that too)\n",
    "\n",
    "n_states = (grid_size[0]*grid_size[1])**n_actors # cartesian product of the number of states s1 x s2\n",
    "discount_factor = 1 # undiscounted, we can even omit it\n",
    "n_batch = 100\n",
    "epochs = 0\n",
    "q_values = np.zeros((n_states,n_actions)) #initialize to zero\n",
    "max_epochs = 1000\n",
    "reward_score = np.zeros(max_epochs)\n",
    "time_score = np.zeros(max_epochs)\n",
    "TS1 = np.zeros(max_epochs)\n",
    "TS2 = np.zeros(max_epochs)\n",
    "while True:\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@\n",
    "    # here starts an epoch\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@\n",
    "    epochs = epochs + 1\n",
    "    reward_progress = np.zeros(n_batch) # bunch of 100 episodes\n",
    "    game_progress = np.zeros(n_batch)\n",
    "    terminal_states = np.zeros((2,n_batch))\n",
    "    eps = 0.5 # starting value of epsilon\n",
    "    # generate an adaptive epsilon greedy algorithm\n",
    "    epsilons = np.array(list(map(lambda i : eps*np.exp(-i/100), np.arange(0,max_epochs+1))))\n",
    "    \n",
    "    for i in range(n_batch):\n",
    "        #@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "        # here starts an episode\n",
    "        #@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "        s_dec = [0,4] # init state: top corners\n",
    "        s_enc = encode(s_dec, 25) # encoded state used to have a scalar index to access the states; 25 singular states\n",
    "        steps = 0\n",
    "        reward = 0\n",
    "        \n",
    "        while True:\n",
    "            steps = steps + 1\n",
    "            \n",
    "            a_enc = e_greedy_pol_v1(s_enc, q_values, eps = epsilons[epochs])\n",
    "            a_dec = decode(a_enc, 4) # 4 singular actions\n",
    "            \n",
    "            sp_dec, r = turn_v2(s_dec, a_dec) # interacts with the environment \n",
    "            sp_enc = encode(sp_dec, 25) # encode to scalar state\n",
    "            reward = reward + r\n",
    "            \n",
    "            a_temp_enc = greedy_pol_v0(sp_enc, q_values)\n",
    "            a_temp_dec = decode(a_temp_enc, 4)\n",
    "\n",
    "            q_values = update_q_v0(s_enc, a_enc, r, sp_enc, a_temp_enc, q_values, gamma = discount_factor)\n",
    "            \n",
    "            # update states\n",
    "            s_enc = sp_enc\n",
    "            s_dec = sp_dec\n",
    "            \n",
    "            \n",
    "            # terminal states are 2 and 24\n",
    "            if (s_dec[0] == 2 and s_dec[1] == 24) or (s_dec[1] == 2 and s_dec[0] == 24):\n",
    "                #print(\"Terminal state reached at step %d.\"%steps)\n",
    "                reward_progress[i] = reward\n",
    "                game_progress[i] = steps\n",
    "                terminal_states[:,i] = s_dec\n",
    "                break\n",
    "            \"\"\"if (s_dec[0] == s_dec[1]):\n",
    "                #print(\"Agent crushed one on each other.\") \n",
    "                #game_progress[i] = 100 # assign 100 equals to failure (and the -100 reward accounts for the accident)\n",
    "                #break\"\"\"\n",
    "            if steps >= 100:\n",
    "                #print(\"Too much time has passed. Game Over.\")\n",
    "                reward_progress[i] = reward\n",
    "                game_progress[i] = steps\n",
    "                terminal_states[:,i] = s_dec\n",
    "                break\n",
    "    \n",
    "    print(\"Average step to solve epoch %d: \"%epochs, game_progress.mean())\n",
    "    print(\"Average reward per episode in epoch %d: \"%epochs, reward_progress.mean())\n",
    "    print(\"Average terminal state for agent 1: \", terminal_states[0].mean())\n",
    "    print(\"Average terminal state for agent 2: \", terminal_states[1].mean(), '\\n')\n",
    "    time_score[epochs-1] = game_progress.mean()\n",
    "    reward_score[epochs-1] = reward_progress.mean()\n",
    "    TS1[epochs-1] = terminal_states[0].mean()\n",
    "    TS2[epochs-1] = terminal_states[1].mean()\n",
    "    \n",
    "    if time_score[epochs-1]  <= 6 and reward_score[epochs-1] >= -6.2:\n",
    "        print(\"Hey champion, you solve it!\")\n",
    "        break\n",
    "    \n",
    "    if epochs >= max_epochs:\n",
    "        print(\"Hey, you're too slow to solve it!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(q_values.max(axis=1).reshape((25,25)))\n",
    "plt.colorbar()\n",
    "plt.title(\"Value map\")\n",
    "#plt.xticks()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminal state reached at step 6.\n"
     ]
    }
   ],
   "source": [
    "s_dec = [0,4] # init state: top corners\n",
    "s_enc = encode(s_dec, 25) # encoded state used to have a scalar index to access the states; 25 singular states\n",
    "steps = 0\n",
    "reward = 0\n",
    "\n",
    "while True:\n",
    "        steps = steps + 1\n",
    "\n",
    "        a_enc = greedy_pol_v0(s_enc, q_values)\n",
    "        a_dec = decode(a_enc, 4) # 4 singular actions\n",
    "\n",
    "        sp_dec, r = turn_v2(s_dec, a_dec) # interacts with the environment \n",
    "        sp_enc = encode(sp_dec, 25) # encode to scalar state\n",
    "        reward = reward + r\n",
    "\n",
    "        a_temp_enc = greedy_pol_v0(sp_enc, q_values)\n",
    "        a_temp_dec = decode(a_temp_enc, 4)\n",
    "\n",
    "        q_values = update_q_v0(s_enc, a_enc, r, sp_enc, a_temp_enc, q_values, gamma = discount_factor)\n",
    "\n",
    "        # update states\n",
    "        s_enc = sp_enc\n",
    "        s_dec = sp_dec\n",
    "\n",
    "\n",
    "        # terminal states are 2 and 24\n",
    "        if (s_dec[0] == 2 and s_dec[1] == 24) or (s_dec[1] == 2 and s_dec[0] == 24):\n",
    "            print(\"Terminal state reached at step %d.\"%steps)\n",
    "            break\n",
    "            \n",
    "        if (s_dec[0] == s_dec[1]):\n",
    "            print(\"Agent crushed one on each other.\") \n",
    "            break\n",
    "            \n",
    "        if steps >= 100:\n",
    "            print(\"Too much time has passed. Game Over.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(time_score[:epochs-1], label = 'MDP')\n",
    "plt.axhline(6, color = 'red', label = 'optimal solution')\n",
    "plt.xlabel(\"epochs\", fontsize = 14)\n",
    "plt.ylabel(\"steps to solve\", fontsize = 14)\n",
    "#plt.ylim(-500,1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEOCAYAAACNY7BQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzsnXd4FVXegN9zWzoJJITepPciKM2CoiK4inUtq651LeyuH/rZd0VXXd1Vd791XV11LeuuvXdFFBULAkrvJUAgEAiE9OTm5nx/nJk7M7ckk5AbSs77PPeZmTNn5p4kML/5dSGlRKPRaDQaE8+BXoBGo9FoDi60YNBoNBqNAy0YNBqNRuNACwaNRqPRONCCQaPRaDQOtGDQaDQajQMtGDQajUbjoMUEgxCimxDiCyHEKiHECiHEb43xWUKIbUKIxcZnakutSaPRaDTRiJZKcBNCdAI6SSl/FEJkAIuA6cB5QJmU8qEWWYhGo9Fo6sXXUl8kpSwACoz9UiHEKqBLU+6Vk5Mje/bs2Yyr02g0msOfRYsW7ZZStm9oXosJBjtCiJ7ASGA+MAGYIYS4BFgI3Cil3BvjmquBqwG6d+/OwoULW2y9Go1GczgghNjsZl6LO5+FEOnAG8ANUsoS4HGgNzACpVE8HOs6KeWTUsrRUsrR7ds3KPA0Go1G00RaVDAIIfwoofBfKeWbAFLKnVLKkJSyDngKOKol16TRaDQaJy0ZlSSAfwGrpJSP2MY72aadCSxvqTVpNBqNJpqW9DFMAC4GlgkhFhtjtwMXCCFGABLIA37VgmvSaDQaTQQtGZU0DxAxTn3YUmvQaDQaTcPozGeNRqPRONCCQaPRaDQODkgeg0ajOUgpWALBSug+1v01UsJP/4Gh54A/Jfp8/kKQdeDxqnv3nAir3ocuo6BN5/rvXReCxS9CRkdI7wCdhqnxrT+Ax6fu0RSKt8LOFdB/inO8ZDv8+G9AwOjLYcMc6H0ibJwLgVQoWAqdhqufs01nNT+jIyx/07rHkLMgd6DaXzcbti+GMVfA0lfBlwR1tep3Mfry2Gtb+S54/bDtR3WcOxCKt0DbHjD4zKb9vI1ECwaNRmPxz2PVdtY+99ds+hLenaGEyrSIyjZSwtMnqv2cflCxB37zE7xyEeQOguu+q//eaz9W9zYx1/Wvkxq/TjtPT4ayHfC7IvDaHoNfPwILnlL7m79RP1tGZyjdHv9eRxyvBAcCkFC0Ds59Dmqr4dVLIFgBeV+re9npfQK07ekcqy6DVy+O/109j4G0HJc/ZNPRgkGjae1U7IE1H8GIC62x/EWw6l1IzoQJv1VvuLHYuxmWvab2dyxV27o6+P4xKN/tfIjtXqu2S19R2+ItartzpdrvPwVWfwDtesPeTZDVA5a/4fy+L+6HY26yjrf+oLQKjxf25UPhKsjuA3s2QMdh0K6X0g4yu6p1+lMhp48SCgBz71fzClcqwbXgKRg0HWrKYf1sNac+oQBKKBxzE5z4O3jpQrWGtZ+oN/5ghZoTKRQAPr4NTvur+j0HK+HIS+GjW6zzx9yk1v3+DdbYC9PhnGchp2/9a9pPtGDQaFo7b18Haz+CTFvpsjmzYNNXar/XsdB1dOxrn50KJflqv3yX2uZ9DZ/eGT1XeMGXDN/9XR37U417TIGqfXBLHrx8YcT8JOc9vnxQmaJMTM0hFh4/1AXVfudRsP0nVFS8ja9jFFo48lIIVsHmbyG5DVSXKgHpS4aT/wAf32oJNZMhZ6lt7gBY8wG8eJ46TmkHJ98LH94ER14G1fuUFrX4v7DmQ8hfYP3eVr2rju3rKLEJpR4T1fniLVowaDSHPWs+Vm+GHYdEnyveCj88qcwOvSdB5V5Y9jqMuRKEEf297Uco26nesIs3Q/9TY3/Pnk2QNw9G2UwVG+cqoQCw6WtrfPO3ypZesES96W+dD6MugR9fgKOugqINsGOZJRQASnfCl39W5h9/GvQ6Ru2bHHEcpLW3NIbyQvVmXWWYgx6L8GvIkPXGDerNfsdSeP5ncX+VDkyhALD9Rzj2f+GrP9d/zdjr1O8a4I44msKAafD+TFj4L3XcfoAyi4G1Nek6BkZepD52Fv9XbU2hAE6h8Ps9SgtKamONXfZB/WtvRrRg0GgONO/9FnqMU3bpSL75Kyx4GjZ+Ab3nwVvXqgd593GWIHlqkvOaeHb3/56r7N/dx6o3zroQ/PsM6/yyV639ulroe7ISDPOfUNtFz8PuNRBIUxpBdYnz/sFy+OJeJRRGXwahGjWe0k45To++Rr0tm4IB1Ju1acM3zTt20jsooQdKazHNVbFo2xNO/D18/ziMuUqtsbzQus/wCyCjE3wwM/492rgs+HzkL5WQ9frhqKstId19rPoOhPo9Tbo99vXjf62czNUl0PcUWPGm+n0ltVGCxzTdpWRB9/Ew6HR362omtGDQaBJF1T71hj32OvDEiAxf8Zayp5fvUvbxNR9Bei50OVKdD9XCirfV/q616kG+/Sd1XFultp/fG33fHcuVzXzYedZYsFIJBYB5f1XRPDn9nNcVb3GaX7qPVQ/5giXqePcatf320WihYOeWPPAF4Ms/qeP0XLh6rrHuGmWWqbIJr9Ltzrf527crTeL1y1QUzvwn1HjbXtY1V30OT51gHfc5CX7xutofcrbabluotK1zn4fB09VYdm+o3BP79wZOc1p9dBoGM36IcX1XuHF1w9effK/6mOxcrqKkZixQUU52Lv/I3ZqaEZ3HoNEkik9/B5/eARs/j33+tV/CP49RJpN9+fDS+c6HXfFmqNgNPSZAqFqZgsy36sq9UFkc2zTyzBR48yooK7TGdthKkC3+j3prXv1+9LVH2yrSdBoR+0FpCphIRv4CJt2hhALYHM+2gge+AIz7dfS1bbrAqX9Wjt9AGvQYDx2GwqhLrTmBVKV1TJypTDa5g5Rg7TgMxl4Tfc8jL1P36D7OOT78AsjsDh2HxlhH19g/W6IZfKYy1UUKhQOE1hg0Gjcsf1PZ8Lsab/NrPlJqf88Jyg+w4i1lHjBNChu/hB+fV/tzH4RV7xn25l+osdoa5/1LbWaU92cqM5Hp/O09SYVOFq6w5lQWwy7jzfSMx+Cd661zNaVqu/Id5Q8Ayxcw8mL46QW1b97fxONXD1PTOZyWox7Yu9eqSJ+i9dbcM/4Bi56DfNtb8xmPOe+XZpTHFxGVcI77X+WE/fZRayyzG/SdDEdfrY4zOsK185zX+dPg1Aet44ZCXTsMir4HqLf6/1mm9mdlRpxrUu+w/efYmxqe04JowaDRNISUyqwBlv3+pfOt4xemq4fmgGnKVCEl/NtmE87/QTk/F78IA3+mTCkVuyO/xNo1nZomPSYCQtm0TSr3WgKg17GQmg0VRc7r8uZZgmHfNrWd+D+WYChcCX0mq7DS1HbQYTC0OwKOmGQlX/WbogTQ8bcp4WdqGbkDrQc/wEn3RP/eUnOs318kNTancnYfK3GtPgKpDc9pLOc+B/OfhPEzYN5flC9Co01JGg2gHLy71ljHNRXw+X3KNl+yLf51oaD1Jm2+wZt+ADsTZyrn4mqjZqQ9GqUh2vZQH/sb/tz7VfZwIEO9bUdGw4CKpwcVPfTpHeqNu90RcI1NwHQ5En71JVz8lrJ5ezxwyduWs3PsNcpmPvQcOP+/1nXt+1umorHXqVyHSOp7kJvRRqc/Cr9epPwQDeFPa3hOYxl8prLhD5gGV34WP1+jlaEFg0ZTWw0f3OiMiV8/G776k3rrNh+wJvY34K02U0rhSrWNJRhGXQzJWZbppT7BkNMPfLbSEqk56sG/y+bUrNyryjVk91amGrMEg0kgQwms2mp4dprxc1aqufY3/cjrGuKsp2DoecoPkNrO+K44D+yc/tBlNJz2l+hzE2cqG/+A09x/dyI0Bk1MtGDQtCy11fDZ3erB1pIUb1Ex9rHMGuWGWcceKWMKg/LdTsHw9SPO2Hr7W3zhKpW5u/CZ6O/I7KY++7bB90/A2k/jrzWrO9xgC8v0BeI/wDMNZ2n7Ac7xvpOVU/vNq5Q9H1S9IlBmJ5NYmkZ9DDsPzjZKRpjCy7xvJP5kuGqOCsWNpH0/pbmYwsUNfi0YWgrtY9C0LIv/C/MeASRMntVy3/vvM2DPRpVoFFm4Ldbbu/n2X74LNtiiiubcbYU+Amz9Xm3TcmH3OitzNzkTeh2n4tzbdFFv6pldYdsiWPeJdf3P/gZbvlfZslX7lLlkygOWfd4k3gM8s5va9j1JvZ1vW6iOj7pa+QRWvqOOOwyx/A1ev9JeghXKtNRUzKzk2uqm36MxxNNMNM2O1hg0LcvezWrrS276PSr2wMe3Ox2YDbFno9rWVqncgFcvUdE/oaClMdgxtYTZv1PJZbmDrXMv27JYtxvNCDuPcCZfpXeAn78A5zyjyiiAingpt4WQegMqRPHMx60wyavnquSzyLwHU2MQEeNmFE1Wd2UjN+kxXsXvm1z6nkrKMklrD9l9lZBoKlowHLZowaBpWUoL1HZ/nHzv36CKtG2Y0/hraypUwtTKd1T0T+GqaI0hWKVKPtiZ/g9r39QmAKqK1bbTcOf8yBo/EJ1V2+4IK5TTfMCntLXOn/A7mGpUK83uq8pMp+Y4bfZ27UcYpaJNgWDXMiJNNiMuUNnJ+8OwnyutaOINDc9tDrQpqcXQpiRNy2I+cCuL65+36j0VEWTP3jVZY9TfiXxTDVbBZ3fBcbc4H4TBKtt+pRIEWd2V36Fkm1MwvHSBSiSTIWts+hNKI7j8E3jmFGs8kKFCRpMznVm5EJ2nAJY/ILMb7NtqZTiDJTRSsqwxe2y7L6CSuTw+9fBf+6kqjeGNEEB2oVGfmeiYG+Ofc0tqO7j03f2/j1u0xtBiaMGgaVnMujcNCYZXjESwSMFQXaaygCH6TX/560obqKuFabaqmfayycFydV2v45Rg2JfvzClYE6MFuWnGsUfzgEpC2/KdeovPjMiYra0iip4Tod+pqjzznD+ouj4mg89UZqL6TDtHXWXVH5pyv5rb+4T4870+lbcQK8P3UOLS92DJK/tn9tI0Cm1K0jSOog3wyR2q5n5TMCN6qiIEQ10IPrrV8gXEw55rEGUCqjTuVWuN7VwJ79hKMFQWKydvh8Eq07dkG5TZ7jPWlkFsYtYUimyQ0sEoYpeabRROg3D5h1iROm06w4Uvq+++8GVn+YMjjoPTHom+xs5RV8E4Y33tjlA+jIZCOCfPsmoHHar0OhamP9bwPE2zoTUGTeN49RJV8GvUJSrJqbGYD+/IcNXda2H+4ypvwF7GoK7O6Yi12/cjBUOdYf7x2P5ZPzdNFU0zMevop+dCm04qfHTrfGWPz+mnzDfdj1ZO5R7jVW8B8+FrL4E8+nKlzexYqmzt7Y6A4ReqhLAFT8dv26jRHAJowaBpHKZdP17sen1IaWkMkYLBvF/JNnjH1sqxci+k2eLuC1ep+Pm2PVVv3r15qojaKfdZFT+F4dj+4SmnUABLMKS1V5FAq99XazrjMauO0aAz1AdUGKiJveaPacu/wpaPcObjanu6rQaQRnMI0ihTkhAiRwhxtBAiRsiFplVhvvk3BruzONLHUFNujO+xavlAtFZQuFJpKkFj/s6Vquhb0QZrrnnuwxiFyYqNcNm09qrtoymo+tTTCczOyfc6w0A1msMQV4JBCJEhhHgVKAS+BboY408IIWYlbnmag5aaMvdzN3yu6t+bD2GPXwkAexZydWnsa+c/oTKFTQpXKbPPPqNaqFm/58XzVO19gJ/+A0tfJSZ789Q2NceWTSwgw2XxtPG/dia4aTSHIW41hgdRwmAUYH9VfB84s7kXpTkEMN/w3bDoOfjmb5aW0WWUitrZ/mPD91v0LPzwT7VfsUf1I8gdABe9BsffrprJTJwZnTD35lUqgcyO8ChfRlIbyOrW+DpBGk0rwa1gOB24QUq5GGc37VXAfuTUaw5KasqVk9l8u45FdQyNYemr8MUfo8cLV6kQU9PeP/RcpTUsf9P2nfVoIGZmsllELneQKhd9/C3qePJdcPHb0deFamCCLfnKFBQDpqkEtPaGYLDXDtJoNK4FQ1ugKMZ4BhCKMa45lNmxXGUG//fc+HNiPcjfvAq+fMAZ/llbbSW1lRhZzxkdVQ6AvVqoqTEMOQdmLFI9e02qS1SSmhmRFOtNP709nHxf9HiyLZLIzC3oM1lt23RWcf6/eCP+z6nRtELcCoYFKK3BxNQafoXyOWgOJ8x6PLvXxq5GCpZg2L1eJaPZ6xatsmXD7l5nZRGbiWb+FFV0zu5YNn0MZ/wdcvrAtIecheQqjCqnSW3iN2wfPyM6CS2QHj3PLBUhhIrz7zwi9v00mlaK23DV24FPhBCDjWtmGvtHAccmanGaA0TIFj1UukPF+4cxBIX5hv/uDJX9O+A9a4o9Sc1esrrEEAy+FPUA32lrVVlTrgSS3VeQ1t7KSi7fZTieB0a3irTzizdUuYgvjEbrgXQ462lVAtrMps7uE/96jUbjTmOQUn4LjAcCwAbgRGA7ME5K+WN912oOMVZ/oPwLJo8MgBfOghfOVCYm04FsvuGbYadvXW1dYzcz2RPSvjT69fpTVG5C+S5LI6kpU7WH7A99e6Zx+W4jVDWi70AknYarnsImgTQYdq5qqWniC0Rfp9FowrhOcJNSLgMuTeBaNAcDZj8BO2YV06oS66FvagxmUhlYJh57hFHhKqvYnIk/VWkDoWolYJLbGIIhokia3SxUtEElu2X3btzPk2QzJf38P01LzNNoWhmuBIMQIgR0klIWRoxnA4VSSt0otTVgNoEB9SCvCzn9BOc8o5LKqsvglYvV+bUfKWfveluvAH+K9dDfs0F1dNu31fkQB6dgKDD6HsTzL8TD7mOwaw0ajSYubp3P8Yy6SUCM+sKaw4JjblKhpbGoKVfmnZDtz99+gNIOqkuUA3rNB9Z97PhTLDPRt39XjXCK1kc7iu2CwWyIE1nFtCFiOZ81Gk291KsxCCFmGrsSuEYIYY9R9ALHAKujLtQkjtKd8OwUuPA1Fb2TSEb+AgqWwLLXos9Vl0aXq0jJUuagovXWWO6g6J6/do1hxVvWeJQpyeZj2GU4sRstGHQNf42msTRkSjLrFQvgSpw5CzVAHnBN8y9LE5cVb6monx/+CVP/nNjv8iXFeFjnqiY3ZYWWYDj+Niu3ICndKlcB1vVnPmk5qH0pqlXmuBnKb7DYKGsR2QVt4OkqKmnVe0pACQ+kd6RRJGU0br5Go6nflCSl7CWl7AV8CQw3j41PfynlKVLK+W6+SAjRTQjxhRBilRBihRDit8Z4OyHEbCHEOmPbtqF7tWrMnACzgmjlXnioP+QvjH9NU/EGotspTnsYuo4xOp8ZoaSDz7KqkQbSoC5ozTdNOcN/bruvT31OuU+1zOx9onGfiOoqadlw7P+q/smgeh54XcZLmD2Utcag0TQat+Gqk6SUexueWS+1wI1SyoHAWOB6IcQg4FZgjpSyLzDHONbEw2xCY/ZM3jJf1Q+a+0Dzf5fXH90IJrOL+tSUKccxOE0+gYg3dPuD+fofYPrj0d9z2iNwyv3OVpd2TLNTfa0qI7nsAzjtr8pspdFoGoXrcFUhRD/gHKA7Kp8hjJSywa4kUsoCoMDYLxVCrEIV5jsDON6Y9jwwF7jF7bpaHZGCwdwmIgzTGwB/xBt3RicrMmj7YtUUJ9nWpzjyDd1uymnfP3Zzn7Y9rc5ksTD7Nzem6F3bnvvf7F6jaaW4DVedBrwB/AQciSqR0RsVlfR1Y79UCNETGAnMBzoYQgMpZYEQIjfONVcDVwN07969sV95+BDZpcwsXyETULLKG3BqDNMeVvWFTAdwwWJVtsLeYS0y5LQ5TDlBo8ZRZrf9v5dGo2kQt+Gq9wB3SynHAdXAxUBP4DPUG75rhBDpKCFzg5SypKH5JlLKJ6WUo6WUo9u3b9/wBYcr8QRDXYRgqK2GP3Z3Rv1sX6zGzNIUDeHxOn0MY65UW1NjKC1ouDZRc4SL+oy+UG177P+9NBpNg7gVDP2BV4z9IJAqpaxCCYwb4l4VgRDCjxIK/5VSmjWXdwohOhnnO6GaAWniYZqSRAOmpPJdUL0PPr7NGlv0rBpb9R6uiXQ+g+qXbJIWUbI6EYLh+Fvh1D/DAJ2gptG0BG4FQylgVjcrAMwAeh+qJHeDCCEE8C9glZTyEdupd7FKbVwKvONyTa0TM6HMjPwxBcTmb+CNq6Lnm6WmwQr1NDWGRwbB1w/X/32+GF1cvX5IMf7skRpDZHhoc9QlSsqAo692mqw0Gk3CcPs/bT4w0dj/AHhYCHEX8Czwnct7TECZoE4QQiw2PlOBB4CThBDrgJOMY008zPaY9v7JJsts7SzN87W2zGRT29iXr4rXlWyDOfdY50O2MFOTeJVMTYEQKRiOOA4m3QFZptmnnkqoGo3moMRtVNJMwLQJzEI16DkbWGucaxAp5TziPyVOdLkOjdn3IKw51DrPz8qE0x+Fd43cRLOE9jvXq17IAMtfV+01TXauhMfHwRWz3a/DdCrbQ1VBvd0fd7Oquvr9Y+7vp9FoDhpcCQYp5UbbfgVwbcJWpKmfoFG51DQRxYpGMoUCWILDFAomK20WuyUvqu3SV3CPIeMjNQaNRnPI48qUJITYaFRSjRzPEkJsjHWNppm4Jxs+m2UdmxqDaSKKjEZqiA5Dof802GpLWK8ygsNimZIaIp5gyDD8GZEahUajOehx62PoiSqaF0kSKklNkwikVG/88/5ijZmNckwTUWMFQ7AiOlFs31a1tRe/s3PV53BthCvJ9D2kxIk9GHudqo807PzGrU+j0RxwGqquepbtcJoQYp/t2IvyDeQlYF0aiO1gDpuSTMFQGz2nPmrKowXDjuVqu/mb2NfEKlWR0w+2LVI9mGPh9TnrI2k0mkOGhnwMrxtbiQo1tRNECYUbm3lNGhN7i8zwWERUUmMznmvKVbkIO+W21JHeJ1od2+pj2sOq8U2HQY37fo1Gc9DTUHVVj5TSA2wBcs1j45NkVFh9v2WW2gqJJRjMcNVQIzSGp0+y9jsMiu6C5k1SGgDAOZHyPw6BNBgwzd1cjUZzSOG2umovKeVu+5iRxaxJJNWxBIPhY6itNnwQERnPHj+c9bRzLP8HtR1wGpz/kjNzGSC7D1zyDlz2UXyfgUajaTW4jUr6jRDibNvxM0ClEGKNECJGuUxNs1BTHj1mhqlunQ8vnR+tMXQYDJ1HxL5f/1NVCQtPRBxBWo4qjtdj/P6vWaPRHPK4jUr6DbALQAhxLHAucCGwGGigpoKmydSURo+ZGgPA2o+jfQzC4yxjkTvY2o/nKE7Jij2u0WhaJW4FQxes6KOfAa9JKV9FZUGPbf5laYBoU1IoGC0IQjXOYyFU60yTURdb+8k2weCxxR0ka8Gg0Wgs3AqGEsDMZDoJ1WkNVGRScswrNOpBHqrHORwK1p9UZpqSzNLalcXRc7b/5DwWHvDb/iRtOlv79gJ3/7NCteQESM503uO672Hm6vjr0mg0hzVuBcOnwFNCiH+hKqt+ZIwPBjYlYmGHBX/IgWdOjn/+rWvgjSvinzejkoQXNn8HD/WJnvPjv53HwuPUGDI6Wft2zSCjI/Q1opU6DXfeI3cgtOmERqNpnbgtonc9cB+qrec5Uso9xvgo4KVELOywYdui+OcKlkT3JK4uU2MeryUYPL7YuQWZ3WHfFueY8KjkMhN7yYrInsnDL1BVULXTWaPR2HAbrloipfy1lPIMKeXHtvG7pJT3J255hzFm2WszL8Ec+9sI+OEpdWz3MdidziaxehpHZimboak9JkSX0BYCesYY12g0rRrd+aS5qNwb/1xkroE5P1hhZTJXFqtQ1PJdsMWoS2T6GGornQJk+uNw3XzwRbh3Tn8UJs9yjgXS4Oq58Is33P8sGo2mVaMFQ3Ow9hN4sCfkzYt9vjxGt9J9+WobLFfN7h/sAR/drMYKV6lt2Q5rfpWtTFVaLuQOUJ3U7HQfFz0G0HlktMlKo9Fo4qAFQ3Ow8Uu1tUcI2aue7tsWfU2JMVZTYWkbi42+CEXroXC1s2dCRZG1b0YdeSPaZsZqw6nRaDSNRAuG5sDMRvbaHsz2yqil26OvMTWGuiBUGNVGzP7NMgT/OFrtDz7TmG8TLr54gkFHDms0mv1HC4bmwCxoZ39jN4UFKK0gFIRSm2moxPagz1+gtiLizzHyF9DHCCk1BQnYBEOE2ShSUGg0Gk0TiBuuKoT4m9ubSCl/0zzLOUSpjSUYbBpDsALm/hG+fhhuWqcihewP+vf/R20jo4NGXw5784z72aKSTH9BpGDQGoNGo2kG6stjGOryHrI5FnJIYwoBe5mJkF0wVELBUrW/6l0Yc2Vsv0OkxtB+AJTujJ4Xy5T026XOjOebN6nwV41Go2kkcQWDlHJSSy7kkCYUo/9ypMaQ1U3tr5+jBENJPvjTrI5sEC0YAmmxeybHMiW17eGck9qucT+DRqPRGGgfQ1OpLIYSw6kcbrMZVLkHxVudPobaKqg2KqWW71J5DSUFkNPXeU/zDf/sf8FNRv/lrmPgqi+URmASLypJo9FomgG3JTEQQvQDzkGVxXA8kaSUlzfzug5+XpiuwlN/v8cSDKEgvHwhbJwLl39izQ1WWoKhci/s2aCESO5AKFhszTPLbKe0hXSjlIUQ0GWU87vNWkhaMGg0mgTgtlHPNGApquT25UB/YCpwJhDD1nGYU7TBylnY/K2zzebGuWp/9zprfrACqkrUfmUxrHhb7Q+aHvv+DT3wPcafzaOb6Gk0mubHrSnpHuBuKeU4oBq4GOgJfAbMTcjKDlbqQvCo7Q1+w+dOjcHEnuwWrHJqDBs+VzWNIk1JJrGylwF6n+hunkaj0ewHbgVDf+AVYz8IpEopq1AC44ZELOygpC4Em75yjlXudfoYTMewQzBUQLVR0kKGoHgzZHaz5vpSnNpDvAf+BS/DLXm2edqUpNFomh+3gqEUqyFPAaonAygfRevpHv/1w8q3YKe61BIM1WWW07lwpdr605SPoarE8g2UbFPtNM2Oakf/ylkeO94D3xdQ/ofwPK0xaDSa5setYJgPTDT2PwAeFkLcBTwLfJeIhR2UbP0heqy6xPIxFG+2xk0BkdKc2TkeAAAgAElEQVTW0BhKIau7dT6lreqodstmOPEuZ5E7t5qA1hg0Gk0CcBuVNBNIN/ZnARnA2cBa41zrJC1XOZPNyqfFW6PnpGRBxR5lQmrbA3avUeNmN7UUY+sQDC41AS0YNBpNAnDbqGejlHKpsV8hpbxWSjlMSnmOlHJLQ9cfcmyZD7VG0lrxFtgTo3vplZ9Dt6OUycjUDszy2ukdrHkpbaHMyF6O1BjsNElj0KYkjUbT/DQ6wU0IkSWEaGf/JGJhB4zd61Wf5k9uU8d/Haq6qgFIW8OdrG6Q1MZqvwlQtss4ZxcAWVb11Oy+znE7/lRrXwsGjUZzAHGbx9BDCPGREKIKKAJ2GZ/dxvaQpKY2Rmc1M3oolj/B3qUtOUv5CEy8Aetau2BIzrT2ex1j7denMXhcWvi0KUmj0SQAtxrDs0AHVHLbicAJxmeSsT0k+MvstZzx2DcArNxeQr87P+KzlRFF6sI5CTXRLTnNN39QEUJmVFFWd8tnILzQprPa96eqqCRQD3u7xpDcDBqDTnDTaDQJwK3z+ShgrJRyeSIXk2iKK2rI262K1v24Rb39z1m9k8mDbD4BM0PZ7L9sIiWU2wQDWBpD7iDYsUztJ2daWkJajqUJZPdRwkR4lSM60pRkP3YtGHSpK41G0/y4fbJsAvarb6QQ4hkhRKEQYrltbJYQYpsQYrHxmbo/39EQPq+H2pDSAqyC1BE9EMwM5doaZ8+EiiIVdgrWgz/J0BhyB1r2/pS21nhSJrTrpfa7j1XbyXepbWpEJZFknZ+g0WgODtwKht8CfxRC9GlwZnyeA6bEGP+LlHKE8flwP+7fIH6vh2DIEAnxehWYfoLaKpj/uDVuVlKd8gDcagRimaak9gMts05Klk0wZMCRv1S9Eab9RY2N/w3cVQwBm+nIvM4ksmGPRqPRtCBuTUnvoDSGNUKIaqDWflJK2aahG0gpvxJC9GzsApsTv1cQrKtjc1E5nxq+hahncLim0R5Y9po1brbltHdJy+mvhEC3o5wag2k+Mk1N9t4I8R76kc5ojUajOUC4FQwzEriGGUKIS4CFwI1Syr0NXdBUfB4PUsKp//c1FTWqqU7UY9r0MZgMPQ+WvQrznzBuYhMMHYfAbUZSmxlJlJylejKApVG4wR69pNFoNAcQV4JBSvl8gr7/ceAPKJP/H4CHUZFPUQghrgauBujevXusKQ3i8yoxYAqFmJgag0m7I9R2wxy19cfpq2zXGEx/wqhL3S/O43U/V6PRaBJIXMEghGgnpdxj7td3E3NeY5FShmNFhRBPAe/XM/dJ4EmA0aNHN6mZccAb7VIJ1UXcqjpCY2jTyXnsSyEmdh9Ddm+Yta8pS9RoNJoDTn0awy4hRCcpZSEqkS3Ww1gY40163TXuX2AcngkkNBzW1BjsRGkPkRpD257OYzcaQ0uS3Qc6DG7Z79Qc9gSDQfLz86mqqmp4suagIzk5ma5du+L3Ny3CsT7BcAJgagKTmnR3G0KIl4DjgRwhRD5wF3C8EGIESrjkAb/a3++pD18MjaGiptY5EOljSM6CX34Az00zbtKAYIhMXEs0v17Ust+naRXk5+eTkZFBz549ETpK7pBCSklRURH5+fn06tWrSfeIKxiklF/G2m8qUsoLYgz/a3/v2xgCbjSGiiKVY2BmOQfSnDWS4gkGTzNoDEPPU/2gNZoDTFVVlRYKhyhCCLKzs9m1q+nVitxGJZlf2A7IJSL/QUq5sskraEF8MTKFw4JBSlj0HOxaDT3Gw2ZDMPhTnYLBH8fH0BympLOfavq1Gk0zo4XCocv+/u3cFtEbKYRYjCqYtwLlC1hm2x4SmD4Gr8f6pYVNSQWL4f0bAAnt+1sXBVKddYziagyGjI0sdaHRaA4prr/+ekaMGMGgQYNISUlhxIgRjBgxgtdff90xr6CggKlTpzJ8+HAGDRrE6aef3uC9586dy+DBgxk5ciQ1NTVNWt+tt95K165dycpK3LPGrcbwDLANlQG9k9iO6IMev+Fj8ApByPgRYoau5tgEgz/NmSWdSI1Bo9EccB577DEA8vLyOO2001i8eHHMeXfeeSfTpk3j+uuvB2Dp0qUN3vs///kPt956KxdffLGrtdTW1uLzOR/TZ5xxBjNmzGDIkCGu7tEU3JbE6Av8Vkr5pZRytZRyjf2TsNU1M6ZgqAlZpqFKUzDUVFgT7RqDLxChMcQpGeU5QM5njeYwJC8vjwEDBnDllVcyZMgQLrroIj777DMmTJhA3759+eEHVRa/vLycyy+/nDFjxjBy5Ejeeeed8PXHHHMMo0aNYtSoUXz77beAemM//vjjOeeccxgwYAAXXXQRMl55nAYoKCiga9eu4eNhw4bVO/+JJ57gzTff5Pe//z2XXHIJdXV1zJw5kyFDhjB06NCwRvLZZ58xefJkzj//fEaOHBl1n3HjxtGxY8cmrdktbjWGecBAYH0C15JwfF7Bhd45rK7rxo+yHwDlpikpaBMM2REloezCIF4eg9evzsULZ9VoDlHufm8FK7eXNDyxEQzq3Ia7flZ/mPX69et57bXXePLJJxkzZgwvvvgi8+bN49133+X+++/n7bff5r777uOEE07gmWeeobi4mKOOOorJkyeTm5vL7NmzSU5OZt26dVxwwQUsXLgQgJ9++okVK1bQuXNnJkyYwDfffMPEiRPrXUssZsyYwYUXXsioUaOYPHkyl112GZ06dYo7/5prrmHevHmcc845TJ8+nVdeeYWVK1eyZMkSdu3axZgxYzj22GMB+P7771m5cmWTk3n3F7eC4QrgaSHEESi/QtB+Ukr5VXMvLBH4PR7u96tAqJsGf0WXrBTO+noa8pVxiCFnqUkeP2RE/HHtjpx4lU+7j4NgZQJWrdG0Tnr16sXQoUMBGDx4MCeeeCJCCIYOHUpeXh4An376Ke+++y4PPfQQoKKptmzZQufOnZkxYwaLFy/G6/Wydu3a8H2POuqo8Jv+iBEjyMvLa5JgmDp1Khs2bODjjz/mo48+YuTIkaxYsYLs7GxX18+bN48LL7wQr9dLx44dmThxIgsXLiQQCDBu3LgDJhTAvWDoC4wATolxrskJbi2NPcEt4POQGvDSw1MIq96B/kbh1xkLwFvPryWet3/EBeqj0RxmNPRmnyiSkixN3ePxhI89Hg+1tUrTl1Lyxhtv0L9/f8e1s2bNokOHDixZsoS6ujqSk5Nj3tfr9Ybv1RSys7O56KKLuOiii5gyZQrz5s3jjDPOcHVtfSastLS0Jq+pOXDrY/gnMAcYigpXbW/75CZmac2P3xaNFPAqwRDGLHwXOLB/EI1G455TTjmFRx99NPyQ/emnnwDYt28fnTp1wuPx8MILLxAK1VMfrYnMmTOHykplJSgpKWHTpk2Ness/9thjefnllwmFQuzcuZNvvvmG0aNHN/s6m4JbwdAVuEtKuUJKuVtKWWT/JHKBzUkAKzwsye8hNWBpBms3G015/KmRl2k0moOU3/3udwSDQYYNG8aQIUP43e9+B8B1113H888/z9ixY1m7dm2zvYE/9thjPP300wAsWLCAUaNGMWzYMMaPH8+1117LyJEjCYVCrh7wpgN8+PDhTJ48mUceeYTc3Ibfs2fOnEnPnj0pKSmha9eu3Hvvvfv9c0Ui3HjkhRBvAy9KKV9t9hU0gdGjR0vTkdQYVm/awoDnlc3ykfE/MKhDClPeUsdvhCZytnce/H6vapn5yR3KnzB5lrp4llEWWxfH07QCVq1axcCBAw/0MjT7Qay/oRBikZSyQanl1sfwMfCwEGIYKqEt0vn8psv7HFACstra93lI91gaxNneeVTKAClmdvQp97X08jQajeagwK1g+IexvT3GuUPG+RyQNlOSz0u6xxlF5KEu8hKLE+9SXd00Go3mMMdtox63voiDGl+dU2NIRR2vb3sMffZ+TZKoJzrhmJmJXp5Go9EcFDT4wBdC+IUQ84UQ/Ruae7DjjzAlpQp1vEF2PlBL0mg0moOOBgWDlDII9OIQrY9kxxeyBEOyJ0SKVE1IZu9quIzFZyt38tZP+Qlbm0aj0RwsuDURPQ9clciFtAQ+m8aQQjUpKMGQV9ehwWuv/PdC/ueVJQlbm0aj0RwsuBUMacDVQojFQoh/CSH+Zv8kcoHNiV1jSKGapDolGEqxchfM8N13Fm/jvSXbW3aBGo3mgHMwl90uLS1l6tSp9O/fn8GDB3PHHXc0+h5ucBuVNBD40dg/IuLcIWNi8tZZ/WuTZSXeWlU4r4Ikvu51A6+tCfJgsI6UgJffvqxK7f5suPY/aDStiYO57LYQgltuuYXjjjuO6upqJk2axOzZsznppJNc3c8trjQGKeWkej4nNOuKEog3ZAmGnKTacEXVCpnMxr6X8W7dhOge0BqNpsXRZbdjl91OT0/nuOOOA1TNp5EjR5Kf3/y+z8a29swBegOLpbQZ7A8RRK215E6pEkpUfaQKkkgx6iZV1IRwVxtRo2klfHQr7GjmRo0dh8KpD9Q7RZfdrr/s9t69e/nwww+5+eabG732hnDb2jNDCPEaUAh8C3Qxxp8QQsxq9lUlCltZ7NRQebhw3uXHDwwX1LvptSXU1TX+DeKrtbv462drG56o0WhcYZbd9ng89ZbdfuCBBxgxYgTHH398uOx2MBjkqquuYujQoZx77rmsXGm1pTfLbns8nnDZ7aZglt2+4oorWLlyJSNHjqSoyH3puHhlt4EGy24Hg0F+/vOfc+ONN9KjR48mrb8+3GoMDwKdgVGopj0m7wP3AbOad1kJotYyJVFVrASDP43/nTKIz1fvBGD+pj3sLouvDAVDdeFOcHY+Wl7AO4u3c8Pkfs2+bI3mgNLAm32i0GW34193xRVXMGTIEGbMmNHoNbvBbVTS6cANUsrFOJ3Nq4h2Rh+82BvpVO6F6n2Q3AaAFL8lI/OKKhyX1dRapTKqa2OXzQiGJBU1IcdcjUaTWFpj2e3bbruNqqqqcHOiROBWMLQFYulIGUDz/8YTRW0Vtb5UJAJ2rYaSAkhSgsHem2FdYal1SajO6gsNVAVj/7i1Rh/pfZXBmOc1Gk3z09rKbufl5fHggw+yfPlyRo0axYgRI3j22Web5Wez47bs9lzgbSnlX4UQpcAwKeUmIcTjQA8p5dRmX1k9NLXsNu/dAKvfh1BQmZIAuoyGq+bw05a9nPkPFbnwy/E9ee7bPACWzjqZ8upaxv3xcwDm3TKJrm2jezZc/+KPfLC0gM9mHkef3PQm/VwazcGCLrt96LM/Zbfdagy3A38QQjyF8kvMFEJ8DlwM3NnI9R44pjygWnfau7QlZQAwtEsml0/oBcD6wrLw6cqaEBUOjSG2qcjSGBqftKLRaDQHE27zGL4FxgMBYANwIrAdGCel/LG+aw8q/MmQ0tZq4wlhweDzevj9zwZxRPs0hympoibkMCVV18YzJSnNa39MSc9/m8efPl7d5Os1Go2mOXCdxyClXAZcmsC1tBzVJda+4Xw2yUlLYuMuS3BU1NS60hhqDI2huKLpguGud1cAcPOUAU2+h0aj0ewvbvMYQkKIKK+IECJbCHHoOJ9NpO3hnuQUDOnJTllZVlXLRU9/Hz6ujut83n+NoSnc+fYynvxqQ4t+p6Z10NSMYM2BZ3//dm59DCLOeBJw6BnVU9pa+75kx6mMCMGwZU8FwZD1S66KZ0qq23+NoSn85/st3P+hNj9pmpfk5GSKioq0cDgEkVJSVFTkyN1oLPWakoQQZtsyCVwjhCiznfYCxwCH3lPpyjnwxpWw/Ueocz7I05Ocv5Id+6ocx9VxTElBm8ZQVFbNRU/P57GLRtG7vY5Q0hx6dO3alfz8fHbt2nWgl6JpAsnJyY46To2lIR/Dr42tAK7EmbNQA+QB1zT52w8U2b1hyNlKMIScWY8ZyX7H8XZDMORmJFFYWt2gxrCvMsicVYWs3lHK3z9fz19+PsLVkppShkOjSRR+v59evXod6GVoDhD1mpKklL2klL2AL4Hh5rHx6S+lPEVKOb9lltrMeANqG3JawiJNSS/9sAWAe6cPAZzO5617KjjjsW/YVVrt8DGkGVpHaZX7VPvSal3VVaPRHBw0puz23kQvpkUZchZ0HAbjrncMm4IhM8XP8G5Wy8+OmcpeZ898Xpq/jyVbi/lh0x6C4aikGmqM9Puyavf+hn0t7JvQaDSaeLh1Ph9+pOXANV8rs5IN08eQ5PPw+9OsrMEObZRguPu9lVz7n0UAlFaph/nanaUOH0OZoSmUVtXy8fIdjLjnUx6dsw6AT1fs4MSH50bVVLJHM2mHn0ajOZC0XsEQB9PHkOT30CkzJTzePj2JgFFV9aPlOwAoM8w/a3eWOmolldgEw9L8Yoorgny8Ql3z65d+YsOu8qgKrsW2jOla7W/QaDQHkBYTDEKIZ4QQhUKI5baxdkKI2UKIdca2bX33aAlMU1LA6yE3w172V5CdHnDMNQXAmp2lBOssjcH0Lewpr6HSMD2tLywjVCfD1VlLqpymI7vGYJqlGkJrFhqNJhG0pMbwHDAlYuxWYI6Usi8wxzg+oKT4VZXVgM+LL6Lvgl0w1IbqwiajvN3lVBjaQzAkKSxVkUxl1bXhchrVtXVs2WOV8470KVRUW76LYK27B749v0Kj0WiaC9eCQQjRQQhxkxDicaPFJ0KICUIIVzFtUsqvgD0Rw2cAzxv7zwPT3a4nUZhNeHpmR1dQbZtqCYaSqtqwj6FOQnlNKFy6O3+v1ffBHpn0wyarcnlxRIZ0pc2pHaxzpzG41Sw0Go2mMbiqlSSEOBL1Rr8JGAz8GdgNnAT0Ay5s4vd3kFIWAEgpC2KV3WhpBnbK4E/nDGPKkI4AvHXdeOoMk43PYyWAmyajgM8TdiTnpCexZU8F22yCYV9lkMwUP/sqg6zeUeoYt2OPdnL7wNeCQaPRJAK3GsNDwP9JKUcCdq/pJ8CEZl9VDIQQVwshFgohFiYyG1MIwXmju9HGcEKP7N6WI3u0A5yt6yY9NJd1haUM7JiB36sEhmlq2lZsCYbiyhraG74KuyYRaUpyaAzalKTRaA4gbgXDkVgmHzsFQIf9+P6dQohOAMa2MN5EKeWTUsrRUsrR7du334+vbD427ConKzVAZooSCF2yUqLmFFcEyU4LIESEYKjHlFSjNQaNRnMAcSsYKlHtPSMZQD0Pcxe8i1XK+1Lgnf24V8Lp3i7a75Ce7CPJ54k6b1qd9lUESQ14SU/ykW9zPhdX1lAVDNHz1g94cf4WRw0mbUrSaDQHEreC4R3gLiGEGb8phRA9gQeBN9zcQAjxEvAd0F8IkS+EuAJ4ADhJCLEO5a94oBFrb3FunzqQu342yDHWxiYY2qT4w/v9OqgGQKXVtST7vbRJ9ofLXuSkB9hXWRuuxPrQp2sczYC0YNBoNAcSt4LhJqAdsAtIBeYB64FiXLb2lFJeIKXsJKX0Sym7Sin/JaUsklKeKKXsa2wjo5YOKpL9Xn4xtodjLCPZT8AQBj6PCOcpDOmSGZ6T4veGM6o9Arq0TaWwpCrscN5TXsMrC7eG58d74N/93gp63vpB+LjGpS9Co9FoGoPbWkklUsqJqHDSW4D/A6ZIKY+TUpbXf/Xhhd/r4YUrjgofj+nZLqwl+G15D8O6WoIhOeANJ861SfEztlc7Fm3eS0FESW+TeE7lZ7/Ji5inNQaNRtP8NCrBTUr5uZTyISnln6SUnyVqUQc7x/RtT44RgXRsvxxLY/AKeuWkAZYpCSDZ5w13hstK8XPasM7U1kk+WLY95v0beuCbJbprXeY7aDQaTWNwm8fw+zinJFCFMit9LKWsjDPvsOPNayewp6KGJJ83LBj8Hg+vXD2WzXsqSPZ5w3NTAp5wDabMFD8DO2XgEbBye4njnqkBLxU1IWpq65BSIoTyYJdUBcNaCags6pSAV5uSNBpNQnAlGIBzge5AGmC+5nYGylF+h25AoRDiOCnlxmZf5UFI9+xUuhvZ0WZxPb9PkNsmmdw2yazdaSWz2X0MWakBfF4PuRnJrC8sc9zTawiCK/+9kDbJfpbcdTKrCko49f++DudCAFTXhkgJeLUpSaPRJAS3pqSHgQVATylldylld6AnMB+4ByUk1gJ/ScQiD3aSDO3A6/HYxqz9ZL+XfUb11HG9swHolJUcLsJnYmZYS6nyHKprQ2w3kuV2lVp5hWazILtgCEVUZK2rk7ornEajaRJuBcNdwEwpZb45YOzfDNwjpSwC7gDGNv8SD35MU1LIZvNPspmSkv1eMpKUKWnqkE4AdI6RDBf5HN+4q9zRMc7EjGayC4bet3/o6PFwzJ++4Kj75zT2R9FoNBrXgqEDkBxjPAkw6xvtRIWytjpM7cD+YLZrDCl+L3ecNpA3rh0fNj91zrR+nVdMVHUI6yLKaH+zfndYCNjrNJl9p2siopfs2dPbiiujej5oNBqNG9wKhs+AfwohxgghPMZnDPA4MNuYMxRVZK/VYWoM1XbB4LcJhoBKcDuyh5U8bm8CNH1EF8BZiwng3g9W8eMW1VHVrmGYWdK1ET4G7XPQaDTNgVvBcCVKI5iPKqJXDXxvjF1lzClFJcK1OgIxNIaALafB3vDHpHOW0hi8HkFOhgp9tfsE/nHRKABeXrDVMR9im5Iiv1+j0WiaiquoJCllITBFCNEf6A8IYJWUcq1tzheJWeLBz8BObQDo2tZ6q7c3+YnlTzDHUv1eMlOU/2Fc72y+Xreb7u1SmTq0E5P6t+eLNaqSbHa6JVyqDAEQaUqq1oJBo9E0A27DVQGQUq4B1iRoLYcs5x7ZlT656YzslhXzfCyNwTQlpQS8pAZ8fPTbY+iUmcyIe2aHk+O62YrymWXAAcqraympChKMEAQ7S6romZ0azn8w2VteQ2aKH4/HOa7RaDSxcC0YhBD9gHNQ+QyO5sdSysubeV2HFEIIRnWP3646skUoQHZagIDXE+76NrBTG+rqJKkBL0O6KA0kNaD+PEk+T7jlKMBvX/6JYEhy66kDHPc8/8nvuXPaQK485ojwWHVtiJF/mM35Y7rxwNnDmv5DajSaVoMrH4MQYhqwFPgZcDnKnDQVOBPISdjqDmM8HkHHzGRSAj7H2LszJvCrY3sDkJ6khIFHCFIC1p/KrKU0f2MRkXyyYofjeHeZyp8wfRWR2PMjNBqNBtw7n+8B7pZSjkM5ni9GJbh9BsxNyMoOEzKS4itlvdunhWsumfTJzSDF0CJMjSFUJxnaJdpMZfof7NRJZ7JbkS1k1d5ZDuDz1TsZc99nfL0ucR3xNBrNoYdbwdAfeMXYDwKpUsoqlMC4IRELOxz47rYTmHfLCXHP//nc4Txy3oi4580yGiEpmTKkI69fMy5qztG92jmOpZSO/tE7SyzBsLe8JrxfWRPi63W7Afh2g1PzKK6ocfSH0Gg0rQu3gqEUK8GtAOhj7PuI3dlNg3IwZ6b6457PSU9y1ECKJNUwJZkawLCu0VrDtGGdHMd10hmdVLDP0hJKqqx2oqf/fV64jHdkm9ER98xmyv99FXddGo3m8Mat83k+MBFYCXwAPCyEGI7yMXyXoLW1etIizFB+r8AjnKUz7FVcwSh3a9MY5m+yeh+V2mozrbMV8IsUDACbiyqixjQaTevArWCYCaQb+7OADOBsVOG8mc2/LA1YpiQTIQTJflWaG1SPaW9ECGqkKemDpQXh/bKIon0mJTEEg0ajab00aEoSQviAAcA2ACllhZTyWinlMCnlOVLKLYleZGvFDGW1Y9Zg6pmdyoe/PSZKMOwqrY7bGW5VQQlVwRAyoibT3ooayqtro5zTGo2mddKgYJBS1gJvorQETQsSqTGAqtQKyn+RnuSLEgwF+6q46On5EdeoP/PT8zZx42tL2GNzQgPk7a5gxos/MuGBz7XTWaPRuHY+L8FyOGtaiNRAtGAwQ1nN+kyRgiEW82+fHN7/YnVhlEZRVl0bDn390yerXa2tNlTHOlszIo1Gc/jgVjDMQjmcpwshugkh2tk/CVxfqyaWxpDaBMFg1mJS1/vI3+t0LGckW99jRio1xJ8/XcNJf/mKzUXlruZrNJpDB7eC4QNUWe03gTxUO89dwG5jq0kAyf7oP0+q3yqTAVY7ULekJXlZu9PZUnSQUQSwZ3bsdhrbiyujMqS/N3If4mVOSylZlr+vUWvTaDQHB26jkiYldBWamJjF8E4a1CE8FmVK8jZOMKT4vayJMAH1yE5l/qY9jO7ZjrwYYarjH/gcgLwHpkWdK40T6fTaonxufn0pT10y2rF+jUZz8OO27PaXiV6IJjbL7z7F0Q3ONCU1VWPYtreS1TtKGdy5DSu2lwBW5dZYUVANEa9L3JodSvhs2l2GagCo0WgOFdyakhBCDBVC/F0I8ZEQopMxNl0IMTJxy9OkJ/nw26qzpoQFg9r6YvgYrjRahYLlgzDNRaXV6g1/Yl+r9uEJA1V31ol9Gl8PMTLCSaPRHPq4ra56MrAA6AKcAJidZ3oDdyVmaZpYRDqfI3ssrLznFO6YNpD3ZkxU8wyh8s6MCVw+QQmMYV0z+d+T+4evGd87h2WzTub4/rm4xewvXRQhGIKhOj5cVsCK7dq/oNEcqrj1MfwBmCml/IcQwm6gngvc2Oyr0sTF7MtgPvDtYqFXTlo4xLVtmjIPmQLE7/WEo5OO7tUu3CNi+ojOAGQk+6MS3+rD1BQiTUnz1u3muv/+2JgfSaPRHGS4FQyDgQ9jjO8BdLhqC2I+6H2G09msm3Rkj7a8cvXY8DzThGT3T5gP8Q5tVD3Etfee6jBFRXZ+g9h9pEN1MiwYisqcGkNxZaQG4V7YxCMYqmP+xj0O85dGo0kcbn0Me1FmpEhGAfnNtxxNQ3g96k9mCoQ64y0/4PU4OsWZDuVLx/cMjx3fvz0Ax/VT24DP02C7z1iZ0HvKa8LfX1Tu1BjKqp3zy6tjRy01hie/2sgv/jWfeUaZcI1Gk1jcCoYXgT8LIbqiCnj6hCY8vDUAACAASURBVBDHAQ8B/07U4jTRmG/4dcaT2SzJHZnolpbkY8P9U7nu+N7hsRMHdmD9fafSt4O76iZ1dZKKoPVg/3a9ejCbpbwzU/xRGkNFhCBoDsGw3ajhtHpHyX7fS6PRNIxbwXAnsAnYjKqyuhL4HJgH3JeYpWliYQqAWkMgmBpDrDd/r0dEmYdi9Z+OR02oLlzJFeDCp+dTVFbN9mJVUmNol0yKymocvolIQVDeDLWXzAzwHXGKA2o0mubF1VNCShmUUl4E9APOAy4EBkgpL5ZS6qprLYgpGEJ1yvYfFgyNS2dwRTBUF2VK2lFSFX6DH9Ilk5pQHWU2YRApCOyCYldpNT/Y+kOYFFfUMHdNYdx1FFeosuD2HhINsbe8JqzhaDSaxuE2XPUMIYRPSrlBSvm6lPJVKeW6RC9OE40vLBjU8YCOKj/h56O7Nft3BUPSoTEAbC+uomBfJUk+D31zVYsOuzmpPo1h2t++5rx/Rvd1uuvdFfzy2QWsL4xdlM/0Y2zY5V4w/PK5BVz49Hyqa/V7i0bTWNxGJb0EVAghXgNekFJ+m8A1aerBI5waQ+eslJilKpqDYKgunK9g8vy3eXg9gk6ZyeQYbUnnbypiX2WQ4d2y4moMC/L2UGjUVaqrkwgBryzYSnFlkNkrdwLw/tICbphs+T+qgiHmrilktyF4dpZUUVcnG3SYAyzZWgxARXUonAyo0Wjc4VYwdADOQZmQvhJCbAH+C/xHSrkmUYvTRHNsPxWyOW1Y54R/V01tHZU1Tg1gnmGeOapXO7LTAgDc8sYyQNVSKq+upUtWSrjpjykYzn3C0hSqakPk763k1jeXOe69MG+v4/j9pQXc9NqScM5GMCTZXVZNbptk3FJWXUtbY50ajcYdbn0MpVLKZ6WUJwHdgL8DpwIrhRA/7O8ihBB5QohlQojFQoiF+3u/w5k+uRnkPTCNo3olPn3km/W7+cl4844kI8lHTnqSY6yippby6lo6ZVoP7tU7Stm021mau7ImFK7KOskIoQUcvgqAwlLlbK4J1dHDqPy6vZEO6EhTmEajaRi3GkMYKWWBEOLvqAilO4Ejm2ktk6SU2lt4EBH5Rm8nNclHu7QAXo8Ih8yuLyyjvKaW3IxkAl4PHg9UBet48CNn85/KYCicbHflMUcwb/1uUvzeKP+E3XcxpEsmm4sqKCiuZES3LNc/Q6Sw0Wg0DeM+dhEQQkwSQjwN7ASeBn4CJtd/leZwYXi3LDKM0NH0JC8Bn4ecdMtMc897KymrqiUtycfa+05l9R9OZUKfbBbkOSORqoJ14Yf+4M5tWHffVE4e3DGGYLCS54Z2yQSaojFY91y8tZjFcTQgjUZj4TYq6c9CiK3Ax0Au8Cugo5TycinlF82wDgl8KoRYJIS4uhnup2kiN53cL2qsnWGj//WkPqQmKUeuWZMpI9nqDrdw817yiipIs5XvHtixTVShvapgiKLyanweEc7QTgt4oxzX9uv6tE8nLeBlSyM7xtmFzfTHvmH6Y9806nqNpjXi1pQ0Afgj8LKUMjoQff+ZIKXcLoTIBWYLIVZLKb+yTzAExtUA3bt3T8ASNAAzTujLkT3accFT3wPw9c2T6NbO6ux234ergGrSwpqD2v7xrKHcZpie0mwtSft1tKKMrp/Um8e+2EBlMERRWQ3t0gLhCKO0JB/l1bVIKcNJebttpqScjCT6dsiI6j4XSf7eCpZstSq7lleHWJC3x1G6PFQnXbVE1WhaK26dz+OllP9IkFBASrnd2BYCbwFHxZjzpJRytJRydPv27SNPa5qRgK3wXpesFMc5M5Ha1ApuPqU/6Uk+ThncMTynoy1qqL+t/IY5XlkTYndZDdk253Vako/aOklNyCraZzclZacF6N8hg7U7Y+c6mFz27AKuf9Gq7lpeU8u5T3zn0BS27InuUqfRaCwa06jHJ4QYL4Q4Xwhxif2zPwsQQqQJITLMfeBkYPn+3FOzf5glLjq0SYqbM2BqBeP75LD87lPC5iaAKUMsIdG3Q3p4v2OmEjKmKcnunzAFzZNfbmRPeQ0vfJfnaAKUnR6gX8cMispr4naN+3TFjqjs6PLq6Kgks7scwOuL8snfqwWFRmPHlSlJCDEAeA/ohWoBEDKuDQLV7F8hvQ7AW4b5wAe8KKX8eD/up9lP+uSm0yUrhb/8fETcOWlJ0Uljt0wZwOKtex2mp9SAj27tUti6pzKsMRRXBFm7o5TpI62CvamGoHl49lr++dVGRzRRit9LasBHP0PIrN1RSk4fZ6hsWXUtV7+wKGpNZdXB8H7A66EmVMd3G3YzZUhHisqquem1JfTNTWf2zOPq/Z1oNK0Jtz6GvwKLgBHADmObCTyOClltMlLKjcDw/bmHpnnJSg3wza0n1DsnLRD9T+daWyVXO/07ZLBjXxXtDA3hw+UFlNeEmDq0U3hOus0vERlimm1cZ5ql1uwsJTPVz4Zd5Zw+vDOvLtwazp62E/B5eOyLDeFjs67UB8t28PufDQ77K9Y3otSGRtMacCsYxgDHSSnLhRB1gE9K+aMQ4mbgUWBYwlaoOSixO5gb4tQhnUjyecPd577fWITfKzjalqSXGohdtuLYfu3p015pCu0zkshK9bN2ZxmPfr6ePeU1DOnchtvfXObwiwDMPKkfj8xe6xirrZN0bZtC/t5K8vdWNOiv0GhaK259DAIwDbG7sJr25AN9mntRmoOfxgiGs4/symMXjQoLhqpgHVmpAUcJ8PQ497vt1AH8/meDANVhrp/hgDZdH1e/sIjaOsmrvxrHRUeraLWzR3XlNyf2jXm/ntlpgAqFXWMIBilhX2Uw5vxIPl6+g0Wb9zY8UaM5hHErGJZjmXt+AG4xGvXcDaxPxMI0BzfmQ74x2NuMZqX4HefMvIjUgNeR2Zyd7qxz1L9DBmt3lJKbofwVJZVBJvVvz+DObcJRThnJ6l5XH3tE1Bq6tVMO8KKyGvJspTrclui+5j+LOPtxXUNSc3jjVjDch9V3/k5UvaQvUBFEv0nAujQHOU1JA/B4BMl+9U8uM0IwmHkFHdok8/b1E8Lj7VKdgqFfxwxKq2tZWVDCuUd25Yc7JvPsZUchhAjf0/Ql3D51YFS+Qte2yjFeVFZNwb4qTh3SkZz0AC/+sIVHZq+lKhhi0eY9vLpgK8u37eOVBVti/izPfrOJjbvKuOX1pXy+eidSSv4xdz3biit56quNbNUhsZpDGFf2ACnlJ7b9jcAgIUQ7YK+0t+/SHPbcf+ZQ7vtgFd2zUxueHINkv9cwJTkFQ4/sVEZ2z+L2qQMd45Ed5+x5EdkRRfz8XiUEgiHrn+SzvxzD9f/9kVLDoW1GTO0uq2Z7cSWTB+bi9+bw7pLtfL1uN36P4OEI38SZI7sS8Hkc5TXufm8lV07sxSsLt7JpdzlH5KTzp4/X8Nw3eRSWVvP24m188JtjGvW70WgOFhpVK8mOlHKPFgqtj7FHZPPeryc2uceBaYJqE6ExJPu9vHXdBMb0rL9qbD9bXkROhJnJ51H/nM1eFaCc1+/MsDSQtql+0pN8rC8so7q2js5ZKeRmWALmya83Rn1nnlGGI7K/9WojH2JPRQ07S1QNJ7PnRCyfRUlVkD9+tIrC0ir++OEqSquCtnuV8Ow3m+r70ZmzaicfL99hrKWaP3+ymlpbQmAsXl2wNWbXvEOJ9YWlPDOv/t+NpnlpdHVVjWZ/aJcWoGBfFVkp9fdIuPXUAVFF9UCF0trvZWfq0I688WM+109yxkPY+zekBry0SwuwbJsqm9EpM4WqoPVwzU4LUFrl/N41O0rp1yEjKrFuVUEJYJml7ERGSQF8tXYX//xyI9+s383ybSWE6iR3nqYc6y/N38Lz323mknE945brePhTpclMGdKRez9YxVs/bWN0z3ZM6p8bc35dneTmN5YCJKyZU0tw5j++pbSqlkvG9WhUz3JN09G/ZU2L0itHRQVF+hgiuea43tx4cv+Y58ww1+QIB3hWaoA3rh1PDyPyyMQe8ZTi95GdHmCj4XjunJXscHDP/d9J/9/emcdXVV17/LsyT5CQhCFAGEIYHRhEBcQJqSIOfVZbK22tUge0z7lOxWfVp9XSPqcKPsdSLCrylKeAqCBQRTGaQJApgRASQhIChMwhuRn2++Oce3NObsKgTfLkru/ncz/3nH32OWff9UnO7+y1916LrY9e5Dr/nnc2UVR+mDsXZbrKvUH+ymob/MYUwtp4gBWXW+KxpdASlDTHm7w3amylo6fxwaYiVtnrM5qaDTkHqimtqWd1VglLNhYCVoY6J69+nktGvnVdb7Kktng/s7DNtR/HQ11DE08s33bMM7q+K16hrm3Q3BqdhQqD0ql4k/h8nxh2f/npaM4d1pOJKQnHfW5UWDCXnJLE0F4xTEiJZ2ivbn4uqejwEN/sJbASBd21KJP8Uv8BZa9rbGtRpavcm6PCSVGF+0G905Hjutg+Vu54yN7+1kZuWGDlrcovrcHTaIUrnzm/JZeV14UFViiTx5dv58oXrWx5ztAfrXtfc9fk8NfV3y9t+7sb9vLK57uZu6ZzJiYe1qRLnYYKg9Kp9LbdOodqPUep2T7J8VH8feYZx5Wy0zuFNTw0iBvOTuGTu87l7ZsmEhkWTEJ0uF997yD3/dNGMCY5zvV2v8oRPsObJ2JzYQWpvVrGPw7VeHj6k2zmrc3hzTRrZlNR+WFX76WuoZnPdhxg7pocX2/irkWZ7K+qo/XwnXcxXmMrwfEKSlmNh/tttxFYfvnb397o23/of7ewJmu/b7+02sPOkmqa2xCwY6XafpNvSwQ7As3G13moMCidys9OT+bCUb258Wz/NQYdyVs3TuDHY/r61j84aT1WATDMFoaE6DCmjGjx4V8xth+9u7cIyZgB1pqLwvLDDOkZzR8uG0VkaDClNR6eX53DnI+y+f2SzRhjKK6oY9zAHvxqwkAuOqk3ANe+/jV//jjb55bKLCjnsaX+7pnsfW2H7fC6oFZuL+Gd9L2+8pc/y6XW0+QTqyUbC7l+/jeA9SA/VOvhcIOVe/u74mm0xmbaGk/pCGo9mo2vs1BhUDqV7hGhvHztePq2Cufd0ZzcL5bnfj62zYHd1ovowCEMMWEMd+SUeObqMa63/olDWtxZw3t34/qzBvPAxSP8rnew2kNReR19YyP4z387mZ+f3n5OkfzSWq772ze+/cam5nbDdxTZ4wg7Wx1fm30AgA8cM7K8lNV68HZIso8zLMiXOQeZt9ZyHdU1Wm/woe34BZuaDY8t3eaX8xvgQFU9s5dspu44xg3UldR5qDAoAU9UWAjXTRrEOzdP9JWdM6wn00/pw7gBPVxrJwBfIiGACYNbhMGblMgZatzLsm+LOFhdz6i+3YG2xcjL5sIKVwrSQ7VW+I7WuTFG9OlG3sEajDFkt0pgtL+qngHxUUSFhbBgppXexOtOc067Pd54UTNeTWPOR9nUNzb5ejXtuXh2lFTx+he7udPh0vLy9MpsFqbt4cPNxcd8b3UldR4qDIoCPHL5SZzhCOoXHx3GvF+cRo/oMFcY8dZEhgX7ckl4exlnDvYfFH965Q6CpCVXRevFeUdiX0UdeQdrmOTonTx86SiuPj2ZstoGDlZ72LHP/wHvbc85w3py/7QRVNU1cqCqnpnzW3oj24oqmb1kM3M+yuKNr/IxxvDIB1vZZAvTh5uL+cWrX/HlroPM+SjLd97ugzU+gXl13W4+2brP7/7eB/nhNnoFQba43rN4EzcuSHet6WiPjhCGjXvKeGpFFlsKK3h82Ta/sZ1ARdcxKMpRCA4S7r1oOEMdg8t/uGyUb2xi0c0TWbA+jxR7Km5wkPDnq07l3v+xBoO7hYcwsk93Th/cwzfGkXCEgfMJKfEM6RnD17sPsXN/Nd/kldHYbJiUmsDiDGscIS4q1JffIj3vEPsq6/yuM25gS8ypvnFW3VfX5fqmsSbHR7Lc8cYeHRbM1JG9mP9lHvO/zCPvqUt4J72AL3JKqalvcvVisvdVuXoeN72R4bdW4lBN+xMMvNF0jYGV20rYVFDB5KGJ7dYHONzwrx9j+NlL62loMry2LpeGJsOs84aQeByifaKiwqAox0DrRXPXnzXYt31yv1jmXOVOKfLT8cmszT7A8s3F/NfPRnOhI/Up+K/BSIwJ8+W4fvsmy6WVs7+aqU//ky/sAH8jk7r76sdGhvpcV7cs3EBbXHpKX992kp0979XPW1YQT0pJZNGhAt9+jaeJdzNaBrCr6hp8s6WcogBwx9uZfqHS563NoXe3CK48rT/gTs3q5D17mquToorDrNpWQmZBOb+7aDiVdQ3MXrKFBx3jNbWeJl5bt5tu4SFcMa4fd76dSVOzISo8mOsnDeaU/rHua5Yf5skVWTxxxcl0j2hZN1NZ18BDS7aQGBPuC5/i/S4ur+sSYViycS8llfXMOrftnCadjQqDonQQD106km4RIZwzrO0c5fdNG87IPt2Z+fdvSIqN5Lfnp7rGEfr3iCQ6LJjVWfsJCRJSEmNIiA6jtMZDXFQoiTHhXHJKkuutH2DBzDNIzy9zxbPyDqA3NRsmpMQzok93zkpNZFF6gevc+V/m+7a3FFb6rb0Aaw1Ks/F37cz5KBuwZm4FBYlvppXTO9PcbPiTwyXlpbi8jvvsHtZtF6Ty1a5Slm4qYpDjNxz2NPH48u0AjE6Oc/1uT2MzL8wY57rmwrR8lm4q4sej+zJ1VG9f+Vtpe/hgU5FfG8ASqNYC0xnM/yKP7cVVXHPGgKMu/uwMVBgUpYNIio3kqSvbz2F163lWL6RHVBhJsRGuXghYvYoLT+rDko2FDEqMJiwkiIEJUZTWeAgLtt7WX5gxluUPuoXhnGE9/cQoNjKUbhEhVNU18uDFIxmdHEd9o/vB3iMq1BX2Y2NBGVV1jaQkRpN7sIaY8BCq6xt5+NJRPLJ0W7u/69K/ruPq05N916qsa+DuRZlcO2kQ9Q1NlFS6exKJMWE8s6olcOGEP35KWa015rDU8QB3hh256NnPXNf4ZFsJt/wjg+vPGszi9AKuGNuPpZssu9ywIJ2JKQk8d80YNhVU8OQKf2Hy3eMIq8WdFByq5Ynl24kOD+GSU/swZYQlPGU1Hv79rQ1EhYUQJDB7+qg2A04uWJ9H5p5y8g/Vct6wnuzcX42nqZlZb2QwcUiCK5/IHz/czpmD47lgZG+/63QUKgyK0sXcNiWVlJ7+M5kAbjh7MJWHG7jIHrR+/pqxvPJZLiOTrB6AiPDKtePZc6jWylVxhFlGC2aewYot+zjVfiMODwlmzpWn+uIpXTG2P687Avl5p7z+auJAdpRUcet5qcxbu4srT+vPsD7d+Cr3EHPX5PgWuInA5aP7smFPGc99utM3lbeksp73NhZS19jkt5jwkctG8VSrHoRXFADyHKvNW7uznHgam1mxZR8r7CCDix0uMYD1uaUs2VB4RFEA/GJetcfCtD18ZA+45x6s9glD2u5Svsgp9dULDwnm+WvG+p3/8PtbfdvOxE/rc0tZn1vKbVNSERFqPY28/FkuL3+W26nxruSHOAo/fvx4k56efvSKiqIclStf/JKM/DJe/tVp3PRGBgDjBsSRWVBOs4HFsya2G/V22EMrfAvd+veIZN39U/h46z5ufiODsJAg3zGAiNAgosJCEKw4U727h5P2+6kMemA5AF8+MIVJT612XT8kSPxWe79y7XhuXHB8///94iKprGvwC5DYFstum0x9YzOPLt1KUmwE+yrq6BsXSUOT4fYLUnl21U6+yTvkutbn951PcnwUz63a6er99IuL5MmfnMKcj7O4clx/aj1NiLS43Vq30TsxYESfbrx54wT2ltVy+QtfADDjzAFk7innsR+fxPijRCFuDxHJMMaMP1o97TEoSoDz7NVj+EdavmuF983nDmFx+l66R4b4wn60xbuzJvHI0q1k5Jf5puCeP7wXV49PprTGQ3x0qG9Fdl1DM3UNHl8+bm8AwIU3nEn2vir6xkUye/pITurXnW1FlYzo053X1uWyxu65AFxyahKTU1tmL70wYyxNzYbgICGruIrcg9X07xFFYdlhxg6IY3ifbuwsqSa1VwxvfJVPRGgwA+IjmbtmFwAXjOjFyf1iyTlQzcD4KOat3cUb6/Op8TTy7d4Kvt1rReHdZH+v2t524MHlm4uZde4QduyvYmBCFNdNGkRGfhnLvi325QPZWZJFvUMo+8VF8rfrT2fhV/kcbmjitilDuXFBOln7qsjaV8U76QWugfA30/YQFxXqN3GhI9Aeg6IoPrxv78fjtlidVcLM+elcPrqvn9uksamZ1NkrAOjTPYKqugb+ed/5jH98FYkx4aQ/NPWI1343Yy/3LN7k29/95HRE5Du104n3/OzHp7lyi9y1KJPlm4sxxpDcI8oXhfdIjO4fy64DNSTFRlBQVsvZQ3vyyrXjqaprYPSjn9BsIKVnNLkH3Ndadfe5rvhaAOW1HsY8thKwFiSGBQf5BvEB1t1/vi8L4XdBewyKohw3r183/pjcLU4mp/bkpnNSuLmNHNshwUHMnTGOZmMIDQ6iur6RxJhwHrh4RLt5JJxMPyWJ7JIqenULJ6VntG/V+QszxhL6PXIzLLttMl/llvolnJp17hAampoREe64YCjvbdjLT8b15/3MQkKDg9hRUsXw3t2IjwljcGI0eQdrGZgQxcI0azbX0N4x/PLMgQB0iwjlkctPIiO/jLumDuOVz3MJDQ6iqdkQEiy+dS9O4qLCuPei4Rhj2Gbn+0jtGUNRRR2DEqK+lygcD9pjUBRFCRCOtcegITEURVEUFyoMiqIoigsVBkVRFMWFCoOiKIriQoVBURRFcaHCoCiKorhQYVAURVFcqDAoiqIoLn6QC9xE5ACQf9SKbZMIHPwXNueHjtqjBbWFG7WHmxPBHgONMW0nCHHwgxSG74OIpB/Lyr9AQe3RgtrCjdrDTSDZQ11JiqIoigsVBkVRFMVFIArDy13dgP9nqD1aUFu4UXu4CRh7BNwYg6IoinJkArHHoCiKohyBgBIGEZkmItkikiMiD3R1ezoaEXldRPaLyBZHWbyIrBSRnfZ3D7tcROR52zbfisi4rmt5xyAiySKyRkS2i8hWEbnDLg84m4hIhIh8LSKbbFs8apcPFpE02xaLRCTMLg+393Ps44O6sv0dhYgEi8hGEVlm7wekPQJGGEQkGJgLXAyMAq4RkVFd26oOZz4wrVXZA8CnxpihwKf2Plh2GWp/bgJe7KQ2diaNwD3GmJHABOC39t9AINqkHphijBkNjAGmicgE4E/AM7YtyoDf2PV/A5QZY1KBZ+x6JyJ3ANsd+4FpD2NMQHyAicDHjv0HgQe7ul2d8LsHAVsc+9lAkr2dBGTb2y8B17RV70T9AO8DPwp0mwBRwAbgTKwFXCF2ue9/BvgYmGhvh9j1pKvb/i+2Q3+sF4MpwDJAAtUeAdNjAPoBBY79vXZZoNHbGFMMYH97E+8GlH3srv9YII0AtYntNskE9gMrgV1AuTHGm/TZ+Xt9trCPVwAJndviDudZ4D6g2d5PIEDtEUjCIG2U6ZSsFgLGPiISA7wL3GmMqTxS1TbKThibGGOajDFjsN6UzwBGtlXN/j6hbSEilwL7jTEZzuI2qgaEPQJJGPYCyY79/kBRF7WlKykRkSQA+3u/XR4Q9hGRUCxRWGiMec8uDmibGGPKgbVY4y5xIhJiH3L+Xp8t7OOxwKHObWmHchZwuYjkAW9juZOeJUDtEUjC8A0w1J5lEAb8HPigi9vUFXwA/Nre/jWWn91bfq09E2cCUOF1r5woiIgArwHbjTFPOw4FnE1EpKeIxNnbkcBUrEHXNcBVdrXWtvDa6CpgtbEd7CcCxpgHjTH9jTGDsJ4Nq40xvyBA7dHlgxyd+QGmAzuwfKmzu7o9nfB73wKKgQasN5zfYPlBPwV22t/xdl3BmrW1C9gMjO/q9neAPSZjdfe/BTLtz/RAtAlwKrDRtsUW4GG7PAX4GsgBFgPhdnmEvZ9jH0/p6t/QgbY5D1gWyPbQlc+KoiiKi0ByJSmKoijHgAqDoiiK4kKFQVEURXGhwqAoiqK4UGFQFEVRXKgwKEonIyKDRMSISEDkD1Z+eKgwKIqiKC5UGBRFURQXKgxKwGGHuLhPRHaJyGER2Swiv7SPed08M0RknYjUiUiWiFzY6hrn2Ala6kSkRESe8SZxcdzjHjvBS72I7BWRJ1s1ZaCdGKhWRLaJyI8c54faSYKK7PMLROSpDjWMotioMCiByONY4UF+i5W06UngJRG5xFFnDvA8VhKblcD7ItIPwP5egRVSYqx9rWvs63j5I/AfdtlJwE9xh/AGeMK+x2isWF5v25FfAW4HrsCK2zMUuBorH4SidDgaEkMJKEQkGiupyoXGmM8d5c8Cw4Bbgd3AQ8aYJ+xjQUAW8I4x5iEReQLrQT3MGNNs17kOK7FPD6wXroNYYb3/u402DLLvMcsY85Jd1g8rntXZxph1IvI8lqBMNfpPqnQyIUevoignFKOwAqB9JCLOB24okOfYX+/dMMY0i0iafS5YeQvWe0XBZh0QBqTa1w/HCsh3JL51bHvDOXuTBM3H6qnsEJFPgA+BFa3uqSgdggqDEmh43aeXAXtaHWug7QQsrRHaT8pijvEa3vtZJxljrKjgVvuMMRvsnsU0rNwAfwc2iciPVByUjkbHGJRAYxtQDww0xuS0+uQ76k3wbth5HM6gJUn8NmCi7WLyMhnwYIXo9t7jgu/TUGNMlTFmsTHmFuASLIFI/T7XVJRjQXsMSkBhjKkSkb8Af7Ef+J8BMVhC0Ax8Yle9RUR2YOVhuBUYCLxoH5sH3AnME5HnsGL2PwW8YIypBbDLnxSRevseCcBpxhjvNY6IiNyNlUsjE6tnMQOoxBqHUJQORYVBCUT+AygBfof1sK/EegDPcdR5ALgbGAfkA1cYY/YCGGMKReRi4M/2eeXApdr2cwAAAHpJREFUm8DvHec/CJTZ9+pv32/BcbSxCrgXa0aSwZoBdbFXeBSlI9FZSYriwDFj6HRjTHrXtkZRugYdY1AURVFcqDAoiqIoLtSVpCiKorjQHoOiKIriQoVBURRFcaHCoCiKorhQYVAURVFcqDAoiqIoLlQYFEVRFBf/B9KUKBrIQL2gAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(TS1[:epochs-1], label = 'mean T.S. for 1')\n",
    "plt.plot(TS2[:epochs-1], label = 'mean T.S. for 2')\n",
    "plt.xlabel(\"epochs\", fontsize = 14)\n",
    "plt.ylabel(\"average terminal state\", fontsize = 14)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
